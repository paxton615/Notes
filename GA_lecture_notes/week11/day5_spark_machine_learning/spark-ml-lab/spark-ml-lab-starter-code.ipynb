{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"> \n",
    "# Spark MLlib Lab\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warning\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding categorical features\n",
    "\n",
    "Often we have categorical features with values given as strings which we would like to transform to numerical values. The analogue of sklearn's `LabelEncoder` is the `StringIndexer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringindexser等于label encoder, 用法和下例相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehotendoderestimator 就是 dummify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_1 = sqlContext.createDataFrame([\n",
    "    (4, \"high\"),\n",
    "    (5, \"low\"),\n",
    "    (6, \"high\"),\n",
    "    (7, \"high\"),\n",
    "    (8,'medium')\n",
    "], [\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = StringIndexer(\n",
    "        inputCol='label',\n",
    "        outputCol='label' + \"_index\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "| id| label|label_index|\n",
      "+---+------+-----------+\n",
      "|  4|  high|        0.0|\n",
      "|  5|   low|        1.0|\n",
      "|  6|  high|        0.0|\n",
      "|  7|  high|        0.0|\n",
      "|  8|medium|        2.0|\n",
      "+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex_2 = string_indexer.fit(ex_1).transform(ex_1)\n",
    "ex_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoderEstimator(\n",
    "        dropLast=True,\n",
    "        inputCols=['label_index'],\n",
    "        outputCols=['label' + \"_index_1\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------+\n",
      "| id| label|label_index|label_index_1|\n",
      "+---+------+-----------+-------------+\n",
      "|  4|  high|        0.0|(2,[0],[1.0])|\n",
      "|  5|   low|        1.0|(2,[1],[1.0])|\n",
      "|  6|  high|        0.0|(2,[0],[1.0])|\n",
      "|  7|  high|        0.0|(2,[0],[1.0])|\n",
      "|  8|medium|        2.0|    (2,[],[])|\n",
      "+---+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onehot.fit(ex_2).transform(ex_2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-hot-encoded values are given as a sparse vector for each observation. The first number indicates the length of the sparse vector, the second number in brackets indicates the position that is filled with the last value. As you can see from the last shown entry, dropping a redundant label (`drop_last`) is default here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the car evaluation dataset \n",
    "\n",
    "```python\n",
    "df = pd.read_csv('../../../../resource-datasets/car_evaluation/car.csv')\n",
    "```\n",
    "\n",
    "Use `acceptability` as target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc\n",
       "2  vhigh  vhigh     2       2    small   high         unacc\n",
       "3  vhigh  vhigh     2       2      med    low         unacc\n",
       "4  vhigh  vhigh     2       2      med    med         unacc"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/paxton615/GA/DSI9-lessons/week11/day5_spark_machine_learning/spark-ml-lab/car_evaluation/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unacc', 'acc', 'vgood', 'good'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.acceptability.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(buying='vhigh', maint='vhigh', doors='2', persons='2', lug_boot='small', safety='low', acceptability='unacc')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df = sqlContext.createDataFrame(df)\n",
    "spark_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buying', 'string'),\n",
       " ('maint', 'string'),\n",
       " ('doors', 'string'),\n",
       " ('persons', 'string'),\n",
       " ('lug_boot', 'string'),\n",
       " ('safety', 'string'),\n",
       " ('acceptability', 'string')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buying', 'string')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.select('buying').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acceptability', 'string')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.select('acceptability').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'acceptability']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[spark_df.dtypes[i][0] for i in range(len(spark_df.dtypes)) if spark_df.dtypes[i][1]=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[buying: string, maint: string, doors: string, persons: string, lug_boot: string, safety: string, acceptability: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummify the categorical variables.\n",
    "\n",
    "Use first the `StringIndexer`, then the `OneHotEncoderEstimator` to create the dummified variables. Be careful not to use one-hot encoding on the target variable (`acceptability`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1 = sqlContext.createDataFrame([(4, \"unacc\"),\n",
    "                                       \n",
    "    (5, \"acc\"),\n",
    "    (6, \"vgood\"),\n",
    "    (7, \"good\"),\n",
    "], [\"id\", \"quality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = StringIndexer( inputCol='quality',\n",
    "                               outputCol='quality'+'_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_2 = string_indexer.fit(target_1).transform(target_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "| id|quality|quality_index|\n",
      "+---+-------+-------------+\n",
      "|  4|  unacc|          0.0|\n",
      "|  5|    acc|          1.0|\n",
      "|  6|  vgood|          3.0|\n",
      "|  7|   good|          2.0|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'acceptability']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoderEstimator(dropLast = True, \n",
    "                                inputCols=['quality_index'],\n",
    "                                outputCols=['quality_index_oh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+----------------+\n",
      "| id|quality|quality_index|quality_index_oh|\n",
      "+---+-------+-------------+----------------+\n",
      "|  4|  unacc|          0.0|   (3,[0],[1.0])|\n",
      "|  5|    acc|          1.0|   (3,[1],[1.0])|\n",
      "|  6|  vgood|          3.0|       (3,[],[])|\n",
      "|  7|   good|          2.0|   (3,[2],[1.0])|\n",
      "+---+-------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onehot.fit(target_2).transform(target_2).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in spark_df.columns:\n",
    "    string_indexer = StringIndexer(\n",
    "    inputCol= i,\n",
    "    outputCol= i + \"_index\"\n",
    "    )\n",
    "    ex_2 = string_indexer.fit(spark_df).transform(spark_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+--------+------+-------------+-------------------+\n",
      "|buying|maint|doors|persons|lug_boot|safety|acceptability|acceptability_index|\n",
      "+------+-----+-----+-------+--------+------+-------------+-------------------+\n",
      "| vhigh|vhigh|    2|      2|   small|   low|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      2|   small|   med|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      2|   small|  high|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      2|     med|   low|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      2|     med|   med|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      2|     med|  high|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      2|     big|   low|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      2|     big|   med|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      2|     big|  high|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      4|   small|   low|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      4|   small|   med|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      4|   small|  high|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      4|     med|   low|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      4|     med|   med|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      4|     med|  high|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      4|     big|   low|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      4|     big|   med|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|      4|     big|  high|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|   more|   small|   low|        unacc|                0.0|\n",
      "| vhigh|vhigh|    2|   more|   small|   med|        unacc|                0.0|\n",
      "+------+-----+-----+-------+--------+------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your feature columns with `VectorAssembler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and evaluate a spark decision tree model and tune with grid search\n",
    "\n",
    "Once done, try also other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
