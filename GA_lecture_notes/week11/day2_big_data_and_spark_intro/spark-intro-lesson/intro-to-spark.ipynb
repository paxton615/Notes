{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Intro to Spark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/spark_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Describe the advantages/disadvantages of Spark compared to Hadoop MapReduce\n",
    "- Define what an RDD is, by its properties and operations\n",
    "- Explain the difference between transformations and actions on an RDD\n",
    "- Implement the different transformations through use cases\n",
    "- Use Spark via python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-0.1\">Learning Objectives</a></span></li></ul></li><li><span><a href=\"#What-is-Spark?\" data-toc-modified-id=\"What-is-Spark?-1\">What is Spark?</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-spark-ecosystem\" data-toc-modified-id=\"The-spark-ecosystem-1.1\">The spark ecosystem</a></span></li><li><span><a href=\"#MapReduce-vs-Spark\" data-toc-modified-id=\"MapReduce-vs-Spark-1.2\">MapReduce vs Spark</a></span></li><li><span><a href=\"#Spark-Visually\" data-toc-modified-id=\"Spark-Visually-1.3\">Spark Visually</a></span></li><li><span><a href=\"#Spark-is-a-distributed-computer-framework-for-parallelized-applications-like-Hadoop.\" data-toc-modified-id=\"Spark-is-a-distributed-computer-framework-for-parallelized-applications-like-Hadoop.-1.4\">Spark is a distributed computer framework for parallelized applications like <em>Hadoop</em>.</a></span></li></ul></li><li><span><a href=\"#Resilient-Distributed-Datasets-(RDD)\" data-toc-modified-id=\"Resilient-Distributed-Datasets-(RDD)-2\">Resilient Distributed Datasets (RDD)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spark-is-an-API-for-handling-large-data-tranformations-like-Map-Reduce.\" data-toc-modified-id=\"Spark-is-an-API-for-handling-large-data-tranformations-like-Map-Reduce.-2.1\">Spark is an API for handling large data tranformations like <em>Map Reduce</em>.</a></span></li></ul></li><li><span><a href=\"#A-&quot;functional-programming-paradigm&quot;-and-DAGs\" data-toc-modified-id=\"A-&quot;functional-programming-paradigm&quot;-and-DAGs-3\">A \"functional programming paradigm\" and DAGs</a></span></li><li><span><a href=\"#Spark-architecture-:-from-your-coding-hands-to-the-cluster\" data-toc-modified-id=\"Spark-architecture-:-from-your-coding-hands-to-the-cluster-4\">Spark architecture : from your coding hands to the cluster</a></span></li><li><span><a href=\"#Spark-Jargon\" data-toc-modified-id=\"Spark-Jargon-5\">Spark Jargon</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spark-has-a-SQL-interface-into-dataframes-like-Hive\" data-toc-modified-id=\"Spark-has-a-SQL-interface-into-dataframes-like-Hive-5.1\">Spark has a SQL interface into dataframes like <em>Hive</em></a></span></li><li><span><a href=\"#Spark-has-a-machine-learning-library-similar-to--Scikit-Learn.\" data-toc-modified-id=\"Spark-has-a-machine-learning-library-similar-to--Scikit-Learn.-5.2\">Spark has a machine learning library similar to  <em>Scikit Learn</em>.</a></span></li><li><span><a href=\"#Spark-is-a-framework-for-building-a-high-volume-stream-processor\" data-toc-modified-id=\"Spark-is-a-framework-for-building-a-high-volume-stream-processor-5.3\">Spark is a framework for building a high volume stream processor</a></span></li></ul></li><li><span><a href=\"#Programming-with-Spark\" data-toc-modified-id=\"Programming-with-Spark-6\">Programming with Spark</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pyspark\" data-toc-modified-id=\"Pyspark-6.1\">Pyspark</a></span></li><li><span><a href=\"#Spark-submit\" data-toc-modified-id=\"Spark-submit-6.2\">Spark-submit</a></span></li></ul></li><li><span><a href=\"#Spark-UI\" data-toc-modified-id=\"Spark-UI-7\">Spark UI</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spark-Jobs\" data-toc-modified-id=\"Spark-Jobs-7.1\">Spark Jobs</a></span></li><li><span><a href=\"#Job-Metrics\" data-toc-modified-id=\"Job-Metrics-7.2\">Job Metrics</a></span></li></ul></li><li><span><a href=\"#Using-Spark-via-Pyspark\" data-toc-modified-id=\"Using-Spark-via-Pyspark-8\">Using Spark via Pyspark</a></span></li><li><span><a href=\"#Initializing-a-SparkContext-in-Python\" data-toc-modified-id=\"Initializing-a-SparkContext-in-Python-9\">Initializing a <code>SparkContext</code> in Python</a></span></li><li><span><a href=\"#RDDs-vs-DataFrames\" data-toc-modified-id=\"RDDs-vs-DataFrames-10\">RDDs vs DataFrames</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-an-RDD-(from-files)\" data-toc-modified-id=\"Creating-an-RDD-(from-files)-10.1\">Creating an RDD (from files)</a></span></li><li><span><a href=\"#Creating-RDDs-from-local-files\" data-toc-modified-id=\"Creating-RDDs-from-local-files-10.2\">Creating RDDs from local files</a></span><ul class=\"toc-item\"><li><span><a href=\"#sc.parallelize()-:-create-an-RDD-from-a-python-array/list\" data-toc-modified-id=\"sc.parallelize()-:-create-an-RDD-from-a-python-array/list-10.2.1\"><code>sc.parallelize()</code> : create an RDD from a python array/list</a></span></li><li><span><a href=\"#sc.textFile()-:-from-a-text-file-!\" data-toc-modified-id=\"sc.textFile()-:-from-a-text-file-!-10.2.2\"><code>sc.textFile()</code> : from a text file !</a></span></li></ul></li></ul></li><li><span><a href=\"#Transformations-:-transforming-an-RDD-into-another\" data-toc-modified-id=\"Transformations-:-transforming-an-RDD-into-another-11\">Transformations : transforming an RDD into another</a></span></li><li><span><a href=\"#Actions-:-turning-your-RDD-into-something-else-(local-object)\" data-toc-modified-id=\"Actions-:-turning-your-RDD-into-something-else-(local-object)-12\">Actions : turning your RDD into something else (local object)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Applying-transformations-and-chaining-them\" data-toc-modified-id=\"Applying-transformations-and-chaining-them-12.1\">Applying transformations and chaining them</a></span><ul class=\"toc-item\"><li><span><a href=\"#.map(func)-:-applying-a-function-on-every-row\" data-toc-modified-id=\".map(func)-:-applying-a-function-on-every-row-12.1.1\"><code>.map(func)</code> : applying a function on every row</a></span></li><li><span><a href=\"#.flatMap(func)-:-applying-a-function-on-every-row-and-flattening-the-resulting-lists\" data-toc-modified-id=\".flatMap(func)-:-applying-a-function-on-every-row-and-flattening-the-resulting-lists-12.1.2\"><code>.flatMap(func)</code> : applying a function on every row and flattening the resulting lists</a></span></li><li><span><a href=\"#.filter(func):-filters-an-RDD-using-a-function-that-returns-boolean-values\" data-toc-modified-id=\".filter(func):-filters-an-RDD-using-a-function-that-returns-boolean-values-12.1.3\"><code>.filter(func)</code>: filters an RDD using a function that returns boolean values</a></span></li><li><span><a href=\"#Now,-let's-see-the-canonical-way-to-write-that-in-Python...\" data-toc-modified-id=\"Now,-let's-see-the-canonical-way-to-write-that-in-Python...-12.1.4\">Now, let's see the canonical way to write that in Python...</a></span></li></ul></li></ul></li><li><span><a href=\"#-Practical-tips\" data-toc-modified-id=\"-Practical-tips-13\"><i class=\"fa fa-thumbs-up\"></i> Practical tips</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sampling-from-enormous-datasets\" data-toc-modified-id=\"Sampling-from-enormous-datasets-13.1\">Sampling from enormous datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#.sample(withReplacement,-fraction,-seed):-sampling-an-RDD-!!\" data-toc-modified-id=\".sample(withReplacement,-fraction,-seed):-sampling-an-RDD-!!-13.1.1\"><code>.sample(withReplacement, fraction, seed)</code>: sampling an RDD !!</a></span></li><li><span><a href=\"#.distinct():-obtaining-distinct-rows\" data-toc-modified-id=\".distinct():-obtaining-distinct-rows-13.1.2\"><code>.distinct()</code>: obtaining distinct rows</a></span></li></ul></li><li><span><a href=\"#Methods-with-a-<key,-value>-paradigm\" data-toc-modified-id=\"Methods-with-a-<key,-value>-paradigm-13.2\">Methods with a <code>&lt;key, value&gt;</code> paradigm</a></span><ul class=\"toc-item\"><li><span><a href=\"#.keys():-returns-the-keys-of-an-RDD-made-of-<k,v>-pairs\" data-toc-modified-id=\".keys():-returns-the-keys-of-an-RDD-made-of-<k,v>-pairs-13.2.1\"><code>.keys()</code>: returns the keys of an RDD made of <code>&lt;k,v&gt;</code> pairs</a></span></li><li><span><a href=\"#.values():-returns-the-values-of-an-RDD-made-of-<key,-value>-pairs\" data-toc-modified-id=\".values():-returns-the-values-of-an-RDD-made-of-<key,-value>-pairs-13.2.2\"><code>.values()</code>: returns the values of an RDD made of <code>&lt;key, value&gt;</code> pairs</a></span></li><li><span><a href=\"#rddA.join(rddB):-join-another-RDD\" data-toc-modified-id=\"rddA.join(rddB):-join-another-RDD-13.2.3\"><code>rddA.join(rddB)</code>: join another RDD</a></span></li><li><span><a href=\"#.reduceByKey(func):-reduce-values-by-their-key-by-applying-func\" data-toc-modified-id=\".reduceByKey(func):-reduce-values-by-their-key-by-applying-func-13.2.4\"><code>.reduceByKey(func)</code>: reduce <code>values</code> by their <code>key</code> by applying func</a></span></li><li><span><a href=\"#.groupByKey(func):-reduce-values-by-their-keys-by-applying-a-function\" data-toc-modified-id=\".groupByKey(func):-reduce-values-by-their-keys-by-applying-a-function-13.2.5\"><code>.groupByKey(func)</code>: reduce <code>values</code> by their <code>keys</code> by applying a function</a></span></li></ul></li><li><span><a href=\"#Sorting-methods\" data-toc-modified-id=\"Sorting-methods-13.3\">Sorting methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#.sortBy(keyfunc):-sorting-by-the-value-of-a-function-on-rows\" data-toc-modified-id=\".sortBy(keyfunc):-sorting-by-the-value-of-a-function-on-rows-13.3.1\"><code>.sortBy(keyfunc)</code>: sorting by the value of a function on rows</a></span></li><li><span><a href=\"#.sortByKey():-sorting-by-key-on-a-<k,v>-RDD\" data-toc-modified-id=\".sortByKey():-sorting-by-key-on-a-<k,v>-RDD-13.3.2\"><code>.sortByKey()</code>: sorting by key on a <code>&lt;k,v&gt;</code> RDD</a></span></li></ul></li><li><span><a href=\"#Actions-that-return-portions-of-an-RDD\" data-toc-modified-id=\"Actions-that-return-portions-of-an-RDD-13.4\">Actions that return portions of an RDD</a></span><ul class=\"toc-item\"><li><span><a href=\"#.collect()-:-returning-the-full-content-of-an-RDD-to-&quot;python-space&quot;\" data-toc-modified-id=\".collect()-:-returning-the-full-content-of-an-RDD-to-&quot;python-space&quot;-13.4.1\"><code>.collect()</code> : returning the <em>full</em> content of an RDD to \"python space\"</a></span></li><li><span><a href=\"#.take(n)-:-returning-(any)-n-lines-of-an-RDD\" data-toc-modified-id=\".take(n)-:-returning-(any)-n-lines-of-an-RDD-13.4.2\"><code>.take(n)</code> : returning (any) n lines of an RDD</a></span></li><li><span><a href=\"#.first()-:-returning-the-first-line-of-an-RDD\" data-toc-modified-id=\".first()-:-returning-the-first-line-of-an-RDD-13.4.3\"><code>.first()</code> : returning the first line of an RDD</a></span></li></ul></li><li><span><a href=\"#Actions-that-compute-some-statistics\" data-toc-modified-id=\"Actions-that-compute-some-statistics-13.5\">Actions that compute some statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#.count()-:-count-the-number-of-lines\" data-toc-modified-id=\".count()-:-count-the-number-of-lines-13.5.1\"><code>.count()</code> : count the number of lines</a></span></li><li><span><a href=\"#.sum():-summing-every-line-in-an-RDD\" data-toc-modified-id=\".sum():-summing-every-line-in-an-RDD-13.5.2\"><code>.sum()</code>: summing every line in an RDD</a></span></li><li><span><a href=\"#.mean():-averaging-every-line-in-an-RDD\" data-toc-modified-id=\".mean():-averaging-every-line-in-an-RDD-13.5.3\"><code>.mean()</code>: averaging every line in an RDD</a></span></li><li><span><a href=\"#.stdev():\" data-toc-modified-id=\".stdev():-13.5.4\"><code>.stdev()</code>:</a></span></li><li><span><a href=\"#.stats():\" data-toc-modified-id=\".stats():-13.5.5\">.stats():</a></span></li></ul></li></ul></li><li><span><a href=\"#Let's-design-chains-of-transformations-together-!\" data-toc-modified-id=\"Let's-design-chains-of-transformations-together-!-14\">Let's design chains of transformations together !</a></span><ul class=\"toc-item\"><li><span><a href=\"#Computing-sales-per-state\" data-toc-modified-id=\"Computing-sales-per-state-14.1\">Computing sales per state</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-RDD\" data-toc-modified-id=\"Input-RDD-14.1.1\">Input RDD</a></span></li><li><span><a href=\"#Task\" data-toc-modified-id=\"Task-14.1.2\">Task</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-14.1.3\">Code</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-14.1.4\">Solution (double click)</a></span></li></ul></li><li><span><a href=\"#Word-count-(again)\" data-toc-modified-id=\"Word-count-(again)-14.2\">Word count (again)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-RDD\" data-toc-modified-id=\"Input-RDD-14.2.1\">Input RDD</a></span></li><li><span><a href=\"#Task\" data-toc-modified-id=\"Task-14.2.2\">Task</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-14.2.3\">Code</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-14.2.4\">Solution (double click)</a></span></li></ul></li><li><span><a href=\"#Find-the-date-on-which-AAPL's-stock-closing-price-was-the-highest\" data-toc-modified-id=\"Find-the-date-on-which-AAPL's-stock-closing-price-was-the-highest-14.3\">Find the date on which AAPL's stock closing price was the highest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-RDD\" data-toc-modified-id=\"Input-RDD-14.3.1\">Input RDD</a></span></li><li><span><a href=\"#Task\" data-toc-modified-id=\"Task-14.3.2\">Task</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-14.3.3\">Code</a></span></li><li><span><a href=\"#Solution\" data-toc-modified-id=\"Solution-14.3.4\">Solution</a></span></li></ul></li></ul></li><li><span><a href=\"#What's-the-in-memory-you-are-talking-about-?\" data-toc-modified-id=\"What's-the-in-memory-you-are-talking-about-?-15\">What's the in-memory you are talking about ?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Caching-/-Persistency\" data-toc-modified-id=\"Caching-/-Persistency-15.1\">Caching / Persistency</a></span></li><li><span><a href=\"#Caching\" data-toc-modified-id=\"Caching-15.2\">Caching</a></span></li><li><span><a href=\"#Persist\" data-toc-modified-id=\"Persist-15.3\">Persist</a></span></li></ul></li><li><span><a href=\"#Spark-data-types\" data-toc-modified-id=\"Spark-data-types-16\">Spark data types</a></span><ul class=\"toc-item\"><li><span><a href=\"#RDD's\" data-toc-modified-id=\"RDD's-16.1\">RDD's</a></span></li><li><span><a href=\"#DataFrames\" data-toc-modified-id=\"DataFrames-16.2\">DataFrames</a></span></li><li><span><a href=\"#Common-DataFrame-operations-and-characteristics\" data-toc-modified-id=\"Common-DataFrame-operations-and-characteristics-16.3\">Common DataFrame operations and characteristics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspect-variable-/-column-space-of-a-DataFrame\" data-toc-modified-id=\"Inspect-variable-/-column-space-of-a-DataFrame-16.3.1\">Inspect variable / column space of a DataFrame</a></span></li><li><span><a href=\"#DTypes\" data-toc-modified-id=\"DTypes-16.3.2\">DTypes</a></span></li><li><span><a href=\"#Explain-DataFrame\" data-toc-modified-id=\"Explain-DataFrame-16.3.3\">Explain DataFrame</a></span></li><li><span><a href=\"#Describe\" data-toc-modified-id=\"Describe-16.3.4\">Describe</a></span></li><li><span><a href=\"#printSchema\" data-toc-modified-id=\"printSchema-16.3.5\">printSchema</a></span></li><li><span><a href=\"#Count\" data-toc-modified-id=\"Count-16.3.6\">Count</a></span></li></ul></li><li><span><a href=\"#Some-basic-stats-in-spark\" data-toc-modified-id=\"Some-basic-stats-in-spark-16.4\">Some basic stats in spark</a></span><ul class=\"toc-item\"><li><span><a href=\"#Covariance\" data-toc-modified-id=\"Covariance-16.4.1\">Covariance</a></span></li><li><span><a href=\"#Pearson-correlation\" data-toc-modified-id=\"Pearson-correlation-16.4.2\">Pearson correlation</a></span></li></ul></li><li><span><a href=\"#Limiting-results\" data-toc-modified-id=\"Limiting-results-16.5\">Limiting results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Head\" data-toc-modified-id=\"Head-16.5.1\">Head</a></span></li><li><span><a href=\"#Show-limited-results\" data-toc-modified-id=\"Show-limited-results-16.5.2\">Show limited results</a></span></li></ul></li><li><span><a href=\"#-Why-should-you-need-to-be-careful-when-displaying-data-in-Spark?\" data-toc-modified-id=\"-Why-should-you-need-to-be-careful-when-displaying-data-in-Spark?-16.6\"><i class=\"fa fa-question-circle\"></i> Why should you need to be careful when displaying data in Spark?</a></span></li><li><span><a href=\"#More-DataFrame-and-Series-operations\" data-toc-modified-id=\"More-DataFrame-and-Series-operations-16.7\">More DataFrame and Series operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-from-list-of-Row-objects-to-list-of-dictionaries\" data-toc-modified-id=\"Convert-from-list-of-Row-objects-to-list-of-dictionaries-16.7.1\">Convert from list of Row objects to list of dictionaries</a></span></li><li><span><a href=\"#Selecting-DataFrame-Series\" data-toc-modified-id=\"Selecting-DataFrame-Series-16.7.2\">Selecting DataFrame Series</a></span></li><li><span><a href=\"#Select-all-features-/-variables-/-columns\" data-toc-modified-id=\"Select-all-features-/-variables-/-columns-16.7.3\">Select all features / variables / columns</a></span></li><li><span><a href=\"#Select-specific-features-/-variables-/-columns\" data-toc-modified-id=\"Select-specific-features-/-variables-/-columns-16.7.4\">Select specific features / variables / columns</a></span></li><li><span><a href=\"#Series-Operations\" data-toc-modified-id=\"Series-Operations-16.7.5\">Series Operations</a></span></li><li><span><a href=\"#As-an-&quot;alias&quot;\" data-toc-modified-id=\"As-an-&quot;alias&quot;-16.7.6\">As an \"alias\"</a></span></li><li><span><a href=\"#Creating-new-features-/-variables-/-columns\" data-toc-modified-id=\"Creating-new-features-/-variables-/-columns-16.7.7\">Creating new features / variables / columns</a></span></li></ul></li><li><span><a href=\"#-Have-we-changed-the-original-DataFrame?\" data-toc-modified-id=\"-Have-we-changed-the-original-DataFrame?-16.8\"><i class=\"fa fa-question-circle\"></i> Have we changed the original DataFrame?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filtering-Data\" data-toc-modified-id=\"Filtering-Data-16.8.1\">Filtering Data</a></span></li><li><span><a href=\"#Multiple-conditions\" data-toc-modified-id=\"Multiple-conditions-16.8.2\">Multiple conditions</a></span></li><li><span><a href=\"#Filter-as-an-expression\" data-toc-modified-id=\"Filter-as-an-expression-16.8.3\">Filter as an expression</a></span></li><li><span><a href=\"#Sorting\" data-toc-modified-id=\"Sorting-16.8.4\">Sorting</a></span></li></ul></li><li><span><a href=\"#AS-SQL!?\" data-toc-modified-id=\"AS-SQL!?-16.9\">AS SQL!?</a></span></li><li><span><a href=\"#Temporary-Views-select-DataFrames\" data-toc-modified-id=\"Temporary-Views-select-DataFrames-16.10\">Temporary Views select DataFrames</a></span></li></ul></li><li><span><a href=\"#Check-out-another-dataset-using-Spark-DataFrames\" data-toc-modified-id=\"Check-out-another-dataset-using-Spark-DataFrames-17\">Check out another dataset using Spark DataFrames</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-up-the-&quot;Pokemon&quot;-basic-Pokedex-dataset\" data-toc-modified-id=\"1.-Load-up-the-&quot;Pokemon&quot;-basic-Pokedex-dataset-17.0.1\">1. Load up the \"Pokemon\" basic Pokedex dataset</a></span></li><li><span><a href=\"#2.-Check-out-the-dataset-with-infer-schema-parameter-but-without-header.\" data-toc-modified-id=\"2.-Check-out-the-dataset-with-infer-schema-parameter-but-without-header.-17.0.2\">2. Check out the dataset with infer schema parameter but without header.</a></span></li><li><span><a href=\"#3.--Create-a-tempory-view-with-the-Pokedex-DataFrame-called-&quot;pokemon&quot;\" data-toc-modified-id=\"3.--Create-a-tempory-view-with-the-Pokedex-DataFrame-called-&quot;pokemon&quot;-17.0.3\">3.  Create a tempory view with the Pokedex DataFrame called \"pokemon\"</a></span></li><li><span><a href=\"#4.a-Which-is-the-strongest-Pokemon-by-Type?\" data-toc-modified-id=\"4.a-Which-is-the-strongest-Pokemon-by-Type?-17.0.4\">4.a Which is the strongest Pokemon by <code>Type</code>?</a></span></li><li><span><a href=\"#4.b-Which-is-the-strongest-Pokemon-by-Type?\" data-toc-modified-id=\"4.b-Which-is-the-strongest-Pokemon-by-Type?-17.0.5\">4.b Which is the strongest Pokemon by Type?</a></span></li><li><span><a href=\"#5.a-Which-Pokemon-has-the-best-combined-Attack-and-Defence?\" data-toc-modified-id=\"5.a-Which-Pokemon-has-the-best-combined-Attack-and-Defence?-17.0.6\">5.a Which Pokemon has the best combined Attack and Defence?</a></span></li><li><span><a href=\"#5.b-Which-Pokemon-has-the-best-combined-Attack-and-Defence?\" data-toc-modified-id=\"5.b-Which-Pokemon-has-the-best-combined-Attack-and-Defence?-17.0.7\">5.b Which Pokemon has the best combined Attack and Defence?</a></span></li><li><span><a href=\"#6.-Create-a-new-feature-called-&quot;Pokevalue&quot;-that-is-the-combined-Attack,-Defense-and-scaled-by-.2-of-the-Pokemon-HP.\" data-toc-modified-id=\"6.-Create-a-new-feature-called-&quot;Pokevalue&quot;-that-is-the-combined-Attack,-Defense-and-scaled-by-.2-of-the-Pokemon-HP.-17.0.8\">6. Create a new feature called \"Pokevalue\" that is the combined Attack, Defense and scaled by .2 of the Pokemon HP.</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-18\">Conclusions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Spark?\n",
    "---\n",
    "\n",
    "\n",
    "- Apache Spark is a (VERY) powerful open source in-memory framework\n",
    "    - It is also called **general purpose processing engine**\n",
    "- It was built on top of Hadoop and originally developed at UC Berkeley in 2009 with the aim of solving some of its problems\n",
    "- The engineers knew EXTREMELY well mapReduce. They learned a lot from:\n",
    "    - why it was so hard to work with\n",
    "    - what were the performance challenges\n",
    "\n",
    "They addressed them very well:\n",
    "- Spark was built around **speed, ease of use, and unified engine** (support libraries for SQL queries, streaming data, machine learning and graph processing)\n",
    "- It is the **largest open source project in data processing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The spark ecosystem\n",
    "\n",
    "![](images/spark_ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce vs Spark\n",
    "\n",
    "\n",
    "**Hadoop MapReduce limits:**\n",
    "- your job has to fit the `<key, value>` paradigm\n",
    "- each job read from disk: problem with iterative algorithms (machine learning)\n",
    "- data is maintained via redundancy\n",
    "\n",
    "**How Spark answers this:**\n",
    "- Spark proposes **other processing workflows than MapReduce**\n",
    "- Highly efficient distributed operations\n",
    "- Spark runs in memory and on disk\n",
    "- Can be up to 100x faster than Hadoop MapReduce in memory, and 10x faster on disk\n",
    "- Spark keeps everything in memory when possible, uses lots of it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Visually\n",
    "\n",
    "- It's a set of tools for developing applications  \n",
    "- It allows parallel processing of applications in a distributed environment\n",
    "\n",
    "![](https://snag.gy/c9b1Kx.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark is a distributed computer framework for parallelized applications like _Hadoop_.\n",
    "\n",
    "_Spark can interact with Hadoop's HDFS to access large amounts of data using high volume, distributed I/O._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDD)\n",
    "\n",
    "<img src=\"images/rdd_on_cluster.png\" width=\"200\" align=\"right\">\n",
    "\\[[Image Source](http://horicky.blogspot.com/2015/02/big-data-processing-in-spark.html)\\]\n",
    "\n",
    "- created from HDFS, S3, HBase, JSON, text, local... or transformed from another RDD\n",
    "- distributed accross the cluster, partitioned (atomic chunks of data)\n",
    "- can recover from errors (node failure, slow process)\n",
    "- traceability of each partition, can re-run the processing\n",
    "- **immutable** : you cannot *modify* an RDD in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark is an API for handling large data tranformations like _Map Reduce_.\n",
    "\n",
    "_It is a data transformation and selection tool like **Pandas**.  You can develop transformations through an API that allows you to chain operations together in a modular fashion, similar to **Pandas**._\n",
    "\n",
    "![](https://snag.gy/9G4gJO.jpg)\n",
    "\n",
    "> However, Spark delivers the idea of data manipulation through the framework of **transformations** and **actions**.  These transformations and actions are performed in parallel, using many different worker nodes (which may be distributed on multiple machines).\n",
    "\n",
    "> Mapreduce is broken down into two main functions \"map\" and \"reduce\".  Spark on the other hand, operates through a set of operations determined through a **Directed Acyclic Graph** (or DAG for short)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A \"functional programming paradigm\" and DAGs\n",
    "\n",
    "\n",
    "Spark provides many transformation functions. By programming these functions, you construct a **Directed Acyclic Graph** (DAG).\n",
    "\n",
    "<img src=\"images/dag.png\">\n",
    "\\[[Image Source]()\\]\n",
    "\n",
    "When you use them, these functions are passed from the **client** to the **master**, who then distributes them to workers, who apply them across their partitions of the RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://snag.gy/ieVW98.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark architecture : from your coding hands to the cluster\n",
    "\n",
    "<img src=\"images/from_rdd_to_cluster.png\">\n",
    "\\[[Image Source]()\\]\n",
    "\n",
    "You construct your sequence of transformations in python.\n",
    "The Spark functional programming interface builds up a **DAG**.\n",
    "This DAG is sent by the **driver** to the **cluster manager** for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Jargon\n",
    "\n",
    "Excerpt taken from \\[[Arush Kharbanda](https://www.quora.com/What-exactly-is-Apache-Spark-and-how-does-it-work) on Quora\\]\n",
    "\n",
    "**Job**: A piece of code which reads some input  from HDFS or local, performs some computation on the data and writes some output data.\n",
    "\n",
    "**Stages**: Jobs are divided into stages. Stages are classified as Map or Reduce stages. Stages are divided based on computational boundaries, not all computations (operators) can be updated in a single Stage. It happens over many stages.\n",
    "\n",
    "**Tasks**: Each stage has some tasks, one task per partition. One task is executed on one partition of data on one executor (machine).\n",
    "\n",
    "**DAG**: DAG stands for Directed Acyclic Graph, in the present context it's a DAG of operators.\n",
    "\n",
    "**Executor**: The process responsible for executing a task.\n",
    "\n",
    "**Driver**: The program/process responsible for running the Job over the Spark Engine\n",
    "\n",
    "**Master**: The machine on which the Driver program runs\n",
    "\n",
    "**Slave/Worker**: The machine on which the Executor program runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark has a SQL interface into dataframes like _Hive_\n",
    "\n",
    "Spark isn't exactly **Hive**, but it uses components from Hive.  You can use temporary SQL views with Spark dataframes.\n",
    "\n",
    ">```python\n",
    "># Load a dataset as a Spark DataFrame\n",
    ">df = spark.read.csv(\"datasets/somedataset/hamburgers_eaten_per_hour.csv\")\n",
    ">df.createOrReplaceTempView(\"hamburgers\")\n",
    ">```\n",
    "\n",
    "\n",
    "\n",
    "Then you can slice and dice your dataframe with SQL:\n",
    "\n",
    ">```python\n",
    ">spark.sql(\"SELECT * FROM hamburgers\").show()\n",
    ">\n",
    "># +------+---------+\n",
    "># | eaten|     name|\n",
    "># +------+---------+\n",
    "># |null  |     Jeff|\n",
    "># |  30  |   Kiefer|\n",
    "># |  19  |     Hang|\n",
    "># +------+---------+\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark has a machine learning library similar to  _Scikit Learn_.\n",
    "\n",
    "![](https://snag.gy/RnuX6h.jpg)\n",
    "\n",
    "_Spark provides an interface to MLib via Scala, Java, Python, and R.  The most common methods are provided such as regression, support vector machines, and random forests, however, not all evaluation metrics are available in Python yet.  Spark is written in Scala, so features are prioritized to Scala first throughout the Spark ecosystem._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark is a framework for building a high volume stream processor\n",
    "![](https://snag.gy/RCikuU.jpg)\n",
    "\n",
    "With **Spark streaming**, it's possible to build a process that can respond to data in **real-time**, using any of Spark's features including Mlib, GraphX, or any kinds of transformations you could do within the Spark context.  The streaming capabilities of Spark core make it possible to produce real-time applications such as **ETL, analytics dashboards, data mining, or large scale aggregations**.\n",
    "\n",
    "In short, you can create a \"streaming context\" that listens on a specific port, and tie to any number of operations that can be programed with spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming with Spark\n",
    "---\n",
    "\n",
    "Running applications built with Spark has a variety of options:\n",
    "\n",
    "- Pyspark\n",
    "- spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark\n",
    "Pyspark is a real-time interpreter for Spark for Python.  You can integrate operations with the Spark Python libraries in real-time.  This is a great way to prototype applications much in the same way we do with Jupyter notebook.  It's also possible to connect Pyspark to Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark-submit\n",
    "Spark-submit, on the other hand, is a way to run a set of application instructions bundled in a single file. It's possible to write these in Python, Scala, or Java.  Typically, it's convenient to prototype a set of operations that you want performed on a dataset with Pyspark, debug them to a high level of quality, then run them with Spark-submit.\n",
    "\n",
    "> With spark-submit, it's also possible to tune the parameters in which your application will run to a very granular degree including memory, number of total cores, and specific cluster modes to control test versus production deployments.\n",
    ">\n",
    "> [Read more about submitting applications](http://spark.apache.org/docs/latest/submitting-applications.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark UI\n",
    "---\n",
    "\n",
    "Anytime a \"spark context\" is created, a corresponding spark UI is launched. It is accessible **only** while the Spark application is running. Through the web UI, you can monitor how your applications run.  Anything that the Spark context handles, even the one line operations from PySpark, can be observed as separate jobs in the Spark UI.\n",
    "\n",
    "Check: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Jobs\n",
    "![](https://snag.gy/JnuSKC.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Metrics\n",
    "![](https://snag.gy/JW2fOb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark via Pyspark\n",
    "---\n",
    "\n",
    "On the terminal, run\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a `SparkContext` in Python\n",
    "\n",
    "IPython / IPython notebook can be a *client* to interact with the *master*.\n",
    "\n",
    "The client will have a `SparkContext` (\"sc\") that..\n",
    "\n",
    "1. Acts as a gateway between the client and Spark master\n",
    "2. Sends code/data from IPython to the master (who then sends it to the workers)\n",
    "\n",
    "<img src=\"https://snag.gy/iCm4G1.jpg\" width=\"600\">\n",
    "\n",
    "**\"sc\"** represents your interface to a running spark cluster manager.  A Spark context is defined as a preconfigured cluster, an application name connected to it.  All **transformations** and **actions** performed by Spark are handled through the Spark context (aka: **sc**).\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "sc = ps.SparkContext('local[4]')\n",
    "```\n",
    "\n",
    "will create a *\"local\"* cluster made of the driver using all your CPU cores (macbook pro has 4 (For each processor core that is physically present (in our case, 2), the operating system addresses two virtual (logical) cores and shares the workload between them when possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warnings\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "sc = ps.SparkContext('local[4]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[2]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if the SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To stop your spark context instance:\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: Spark does some very heavy lifting for us and works very well most of the time.\n",
    "It is not perfect though. If things start getting ugly (understand pyspark unable to create a spark context connection) run in terminal:\n",
    "\n",
    "```\n",
    "ps -ax | grep spark\n",
    "kill <PID>\n",
    "```\n",
    "\n",
    "This is quite an extreme measure. Use it only if nothing else works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs vs DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "The two main types of data objects in Spark are the **Resilient Distributed Dataset** and the **DataFrame**.  Both types represent data in a distributed state.  RDDs store data in a more primitive state such as a list of pairs, integers, floats, or strings.  DataFrames have a rich structure defintion called a **schema** much like a Pandas dataframe.\n",
    "\n",
    "- You use **RDDs** to manage semi-structured data.\n",
    "- You use **DataFrames** to operate on typed series.\n",
    "\n",
    "Both RDDs and DataFrames can contain multiple types of objects.  **DataFrames** are much more constrained because data is represented by a two-dimensional tabular structure where columns represent variables and rows observations.  **RDDs** are much more flexible if your data requires much less structure than a DataFrame while still being able to use _transformation_ methods such as **`map()`** and _action_ methods such as **`reduce()`**.\n",
    "\n",
    ">_Distributed Data in Spark_\n",
    ">![](https://snag.gy/vxVhri.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an RDD (from files)\n",
    "\n",
    "RDDs are **immutable**. Once created, you cannot modify them directly. You can only transform them into another RDD. \n",
    "\n",
    "Functions for creating an RDD from an external source are methods of the SparkContext object `sc`.\n",
    "\n",
    "| Method | Description |\n",
    "| - | - |\n",
    "| `sc.parallelize(array)` | Create an RDD from a python array or list |\n",
    "| `sc.textFile(path)` | Create an RDD from a text file |\n",
    "| `sc.pickleFile(path)` | Create an RDD from an HDFS pickle file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs from local files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.parallelize()` : create an RDD from a python array/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ps.SparkContext('local[4]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = ([['isaac', 18],\n",
    "              ['lee', 7],\n",
    "              ['brad', 2],\n",
    "              ['giovanna', 14],\n",
    "              ['darren', 10],\n",
    "              ['cary', 42]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['isaac', 18],\n",
       " ['lee', 7],\n",
       " ['brad', 2],\n",
       " ['giovanna', 14],\n",
       " ['darren', 10],\n",
       " ['cary', 42]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['isaac', 18],\n",
       " ['lee', 7],\n",
       " ['brad', 2],\n",
       " ['giovanna', 14],\n",
       " ['darren', 10],\n",
       " ['cary', 42]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the results (lazy)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop your spark session, run:\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.textFile()` : from a text file !\n",
    "\n",
    "The import will give you an rdd made of **strings which are lines of the text file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthew,4\r\n",
      "jorge,8\r\n",
      "josh,15\r\n",
      "evangeline,16\r\n",
      "emilie,23\r\n",
      "yunjin,42\r\n"
     ]
    }
   ],
   "source": [
    "cat 'data/toy_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthew,4\n",
      "jorge,8\n",
      "josh,15\n",
      "evangeline,16\n",
      "emilie,23\n",
      "yunjin,42\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['matthew,4', 'jorge,8', 'josh,15', 'evangeline,16', 'emilie,23', 'yunjin,42']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = ps.SparkContext('local[4]')\n",
    "\n",
    "# displaying the content of the file in stdout\n",
    "with open('data/toy_data.txt', 'r') as text:\n",
    "    print(text.read())\n",
    "\n",
    "# reading the file using SparkContext\n",
    "rdd = sc.textFile('data/toy_data.txt')\n",
    "\n",
    "# to output the content in python (use collect() with great care)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/toy_data.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop your spark session, run:\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations : transforming an RDD into another\n",
    "\n",
    "- They are **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**.\n",
    "- They transform an RDD into another RDD because RDDs are **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "<img src=\"images/rdd_narrow_vs_wide_transformations.png\" width=\"400\"/>\n",
    "\\[[Image Source](http://horicky.blogspot.com/2013/12/spark-low-latency-massively-parallel.html)\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - |-|\n",
    "| [`.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) | transformation | mapping | Return a new RDD by applying a function to each element of this RDD. |\n",
    "| [`.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) | transformation | mapping | Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. |\n",
    "| [`.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter) | transformation | reduction |  Return a new RDD containing only the elements that satisfy a predicate. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample) | transformation | reduction | Return a sampled subset of this RDD. |\n",
    "| [`.distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct) | transformation | reduction |  Return a new RDD containing the distinct elements in this RDD. |\n",
    "| [`.keys()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys) | transformation | `<k,v>` | Return an RDD with the keys of each tuple. |\n",
    "| [`.values()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values) | transformation | `<k,v>` | Return an RDD with the values of each tuple. |\n",
    "| [`.join(rddB)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) | transformation | `<k,v>` | Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. |\n",
    "| [`.reduceByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) | transformation | `<k,v>` | Merge the values for each key using an associative and commutative reduce function. |\n",
    "| [`.groupByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) | transformation | `<k,v>` | Group the values for each key in the RDD into a single sequence. |\n",
    "| [`.sortBy(keyfunc)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy) | transformation | sorting |  Sorts this RDD by the given keyfunc. |\n",
    "| [`.sortByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey) | transformation | sorting/`<k,v>` | Sorts this RDD which is assumed to consist of (key, value) pairs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions : turning your RDD into something else (local object)\n",
    "\n",
    "Actions are specific methods of an RDD object, they are usually designed to transform an RDD into something else (e.g. a python object or a statistic).\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | Type | Description |\n",
    "| - | - | - |\n",
    "|[`.reduce()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce)| action | Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.|\n",
    "| [`.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) | action | Return a list that contains all of the elements in this RDD. Note that this method should only be used if the resulting array is expected to be small as all the data is loaded into the driver’s memory. |\n",
    "| [`.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) | action | Return the number of elements in this RDD. |\n",
    "| [`.take(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) | action | Take the first `n` elements of the RDD. |\n",
    "| [`.top(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top) | action | Get the top `n` elements from a RDD. It returns the list sorted in descending order. |\n",
    "| [`.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first) | action | Return the first element in an RDD. |\n",
    "| [`.sum()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sum) | action | Add up the elements in this RDD. |\n",
    "| [`.mean()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mean) | action | Compute the mean of this RDD’s elements. |\n",
    "| [`.stdev()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.stdev) | action | Compute the standard deviation of this RDD’s elements. |\n",
    "| [`.countByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)|action| Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying transformations and chaining them\n",
    "\n",
    "Recall the spark flow:\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "We'll proceed along the usual spark flow (see above):\n",
    "1. Create the environment to run spark from python\n",
    "2. Extract RDDs from files\n",
    "3. Run some transformations\n",
    "4. Execute actions to obtain values (local objects in python)\n",
    "\n",
    "Each transformation is a method of an RDD and returns another RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID    Date           Store   State  Product    Amount\n",
      "101    11/13/2014     100     WA     331        300.00\n",
      "104    11/18/2014     700     OR     329        450.00\n",
      "102    11/15/2014     203     CA     321        200.00\n",
      "106    11/19/2014     202     CA     331        330.00\n",
      "103    11/17/2014     101     WA     373        750.00\n",
      "105    11/19/2014     202     CA     321        200.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/sales.txt', 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recall: Input functions, reading RDDs from files, are functions of the SparkContext.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ps.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ps.SparkContext('local[4]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ID    Date           Store   State  Product    Amount',\n",
       " '101    11/13/2014     100     WA     331        300.00',\n",
       " '104    11/18/2014     700     OR     329        450.00',\n",
       " '102    11/15/2014     203     CA     321        200.00',\n",
       " '106    11/19/2014     202     CA     331        330.00']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads a text file line by line\n",
    "rdd1 = sc.textFile('data/sales.txt')\n",
    "\n",
    "rdd1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.map(func)` : applying a function on every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies split() to each row\n",
    "rdd2 = rdd1.map(lambda row: row.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#ID', 'Date', 'Store', 'State', 'Product', 'Amount'],\n",
       " ['101', '11/13/2014', '100', 'WA', '331', '300.00']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.flatMap(func)` : applying a function on every row and flattening the resulting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'State', '100', 'WA', '700', 'OR', '203', 'CA', '202', 'CA']\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd\n",
    "out_rdd = rdd2.flatMap(lambda x: (x[2], x[3]))\n",
    "print(out_rdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.filter(func)`: filters an RDD using a function that returns boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filters rows\n",
    "rdd3 = rdd2.filter(lambda row: not row[0].startswith('#'))\n",
    "\n",
    "rdd3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casting_function(X):\n",
    "    return int(X[0]), X[1], int(X[2]), X[3], int(X[4]), float(X[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casting_function(['101', '11/13/2014', '100', 'WA', '331', '300.00'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies casting_function to rows\n",
    "rdd4 = rdd3.map(casting_function)\n",
    "\n",
    "# shows the result\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's see the canonical way to write that in Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt')\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda rowstr: rowstr.split())   # <= JUST ADDED THIS HERE\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda rowstr: rowstr.split()) \\\n",
    "              .filter(lambda row: not row[0].startswith('#'))    # <= JUST ADDED THIS HERE\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda rowstr: rowstr.split()) \\\n",
    "              .filter(lambda row: not row[0].startswith('#')) \\\n",
    "              .map(casting_function)   # <= JUST ADDED THIS HERE\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda x: x.split()) \\\n",
    "              .filter(lambda x: not x[0].startswith('#')) \\\n",
    "              .map(casting_function)\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-thumbs-up\" aria-hidden=\"true\"></i> Practical tips\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from enormous datasets\n",
    "Undoubtably, you may have the need to examine a larger dataset.  A common operation is to take a sample.  To approximate the characteristics of your global distribution, you should try to adjust the size that best matches the metrics of central tendency or consider doing a power analysis to determine sample sizing.\n",
    "\n",
    "> Size of your sample generally depends on your application be it A/B testing, EDA, Machine Learning, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sample(withReplacement, fraction, seed)`: sampling an RDD !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling an rdd\n",
    "out_rdd = sales_rdd.sample(True, 0.4)\n",
    "out_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.distinct()`: obtaining distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining distinct values of the \"state\" column of rdd_sales\n",
    "out_rdd = sales_rdd.map(lambda x: x[3]) \\\n",
    "                   .distinct()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: \\n{}\".format(sales_rdd.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"\\n\" + \"after: \\n{}\".format(out_rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods with a `<key, value>` paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset of the rdd to be used as key-value pairs\n",
    "sales_subset = sales_rdd.map(lambda x: (x[3], x[-1]))\n",
    "sales_subset.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.keys()`: returns the keys of an RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_subset.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.values()`: returns the values of an RDD made of `<key, value>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_subset.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rddA.join(rddB)`: join another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating an adhoc list for each state\n",
    "managers_array = [['CA', 'Hot'],\n",
    "                  ['OR', 'Cold'],\n",
    "                  ['WA', 'Boring'],\n",
    "                  ['TX', 'Gun']]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "managers_rdd = sc.parallelize(managers_array)\n",
    "\n",
    "# to output the content in python (use with great care)\n",
    "sales_subset.join(managers_rdd).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.reduceByKey(func)`: reduce `values` by their `key` by applying func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_subset.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.groupByKey(func)`: reduce `values` by their `keys` by applying a function \n",
    "This can use any function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_func(iterator):\n",
    "    total, count = 0, 0\n",
    "    for x in iterator:\n",
    "        total += x\n",
    "        count += 1\n",
    "    return total / count\n",
    "\n",
    "\n",
    "sales_subset.groupByKey() \\\n",
    "    .map(lambda x: (x[0], mean_func(x[1]))) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortBy(keyfunc)`: sorting by the value of a function on rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd.sortBy(lambda x: x[3], ascending=True).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortByKey()`: sorting by key on a `<k,v>` RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting k,v pairs by key\n",
    "sales_subset.sortByKey(ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions that return portions of an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.collect()` : returning the *full* content of an RDD to \"python space\"\n",
    "\n",
    "Returns the rows of an RDD as a list. Can be a bad idea if your RDD is gigantic because `.collect()` will return everything and put it in memory for python to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.take(n)` : returning (any) n lines of an RDD\n",
    "\n",
    "Returns `n` rows of an RDD as a list. These `n` are not randomly selected. They are Spark's own internal mechanism for obtaining the lines that can be collected first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.first()` : returning the first line of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions that compute some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.count()` : count the number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sum()`: summing every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sales_subset.values().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.mean()`: averaging every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_subset.values().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.stdev()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_subset.values().stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .stats():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_subset.values().stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Let's design chains of transformations together !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing sales per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def casting_function(x):\n",
    "    (_id, date, store, state, product, amount) = x\n",
    "    return((int(_id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda x: x.split()) \\\n",
    "              .filter(lambda x: not x[0].startswith('#')) \\\n",
    "              .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "You want to obtain a sorted RDD of the states in which you have most sales done (amount).\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "If you had to draw a workflow of the transformations to apply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rdd = sales_rdd  # apply transformation here...\n",
    "\n",
    "out_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rdd = rdd_sales.map(lambda x: (x[3], x[5]))   \\\n",
    "    .reduceByKey(lambda x,y: x + y) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)  \n",
    "out_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "out_rdd = rdd_sales.map(lambda x: (x[3], x[5])) \\\n",
    "                   .reduceByKey(lambda x,y: x + y) \\\n",
    "                   .sortBy(lambda x: x[1], ascending=False)\n",
    "<br/>\n",
    "out_rdd.collect()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rdd = rdd_sales.map(lambda x: (x[3], x[5])) \\\n",
    "                   .reduceByKey(lambda x, y: x + y) \\\n",
    "                   .sortBy(lambda x: x[1], ascending=False) \n",
    "out_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/input.txt', 'r') as fin:\n",
    "    print(fin.read())\n",
    "\n",
    "# reading the file using SparkContext\n",
    "\n",
    "rdd = sc.textFile('data/input.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "What transformations do you need to apply in order to do the word count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(lambda row: row.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rdd = rdd2.flatMap(lambda x: (x[2], x[3]))\n",
    "print(out_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rdd = rdd  # apply transformation here...\n",
    "\n",
    "# collect the result\n",
    "out_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_rdd = rdd.flatMap(lambda x : x.split()) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x + y) \n",
    "out_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "out_rdd = rdd.flatMap(lambda x : x.split()) \\\n",
    "             .map(lambda x: (x, 1)) \\\n",
    "             .reduceByKey(lambda x, y: x + y)\n",
    "<br/>\n",
    "out_rdd.collect()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the date on which AAPL's stock closing price was the highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl_raw_rdd = sc.textFile('data/aapl.csv')\n",
    "\n",
    "print(\"lines in file: {}\".format(appl_raw_rdd.count()))\n",
    "\n",
    "appl_raw_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "1. filter out headers\n",
    "2. split each line based on comma\n",
    "3. keep only fields for Date (col 0) and Close (col 4)\n",
    "4. order by Close in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rdd = appl_raw_rdd  # apply transformation here...\n",
    "\n",
    "out_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "out_rdd = appl_raw_rdd.filter(lambda x: not x.startswith(\"Date\")) \\\n",
    "                      .map(lambda x: x.split(\",\")) \\\n",
    "                      .map(lambda x: (x[0], float(x[4]))) \\\n",
    "                      .sortBy(lambda x: x[1], ascending=False)\n",
    "<br/>\n",
    "out_rdd.collect()<br/>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the in-memory you are talking about ?\n",
    "\n",
    "Recall:\n",
    "- Spark runs in memory and on disk\n",
    "- Can be up to 100x faster than Hadoop MapReduce in memory, and 10x faster on disk.\n",
    "- Spark keeps everything in memory when possible, uses lots of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching / Persistency\n",
    "\n",
    "- The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "- If you have an RDD that you are going to reuse in your computation you can use cache() to make Spark cache the RDD.\n",
    "- This is especially useful if you have to run the same computation over and over again on one RDD like in common machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Consider the following job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_count = 5*10**5\n",
    "num_list = [random.random() for i in range(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again.\n",
    "- Calling cache() flips a flag on the RDD.\n",
    "- The data is not cached until an action is called.\n",
    "- You can uncache an RDD using unpersist()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist\n",
    "\n",
    "- Persist RDD to disk instead of caching it in memory.\n",
    "- You can cache RDDs at different levels.\n",
    "\n",
    "| Level\t| Meaning |\n",
    "| - | - |\n",
    "| MEMORY_ONLY\t| Same as cache() |\n",
    "| MEMORY_AND_DISK\t| Cache in memory then overflow to disk |\n",
    "| MEMORY_AND_DISK_SER\t| Like above; in cache keep objects serialized instead of live |\n",
    "| DISK_ONLY\t| Cache to disk not to memory |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark data types\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD's\n",
    "\n",
    "It's best to think of RDDs as primitive objects that are distributed.  RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "The big difference between RDD's and DataFrames is that DataFrames introduce the idea of a \"schema\" much like Pandas.  \n",
    "\n",
    "The big plus is that Spark DataFrames serialize data at a lower level to native Java/Scala, so when it's passed between nodes, it's much more performant, requiring fewer processes to handle computations.  Mainly data can be processed faster when it's optimized to a common format (the schema) that Spark doesn't have to convert to in order to perform tasks on it.\n",
    "\n",
    "Outside of the performance optimizations introduced with a schema-based datastructure, the **DataFrame API** provides a convenient set of selectors for transforming data, much like Pandas.  Lastly, it's possible to create temporary views in which **DataFrames** can be queried with SQL - **SparkSQL**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "sc = ps.SparkContext(\"local\", appName=\"myAppNameSolution\")\n",
    "spark = ps.sql.SparkSession(sc)\n",
    "\n",
    "df = spark.read.csv(\n",
    "    path=\"data/sentiment_words_simple.csv\",\n",
    "    header=True,\n",
    "    # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    # Not always perfect but works well in most cases as of 2.1+\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common DataFrame operations and characteristics\n",
    "---\n",
    "\n",
    "Let's have a look at some familiar and new functions and properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect variable / column space of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTypes\n",
    "\n",
    "Inspect schema programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain DataFrame\n",
    "Show details about DataFrame type, schema, and origin of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe\n",
    "Describe will look similar to the synonymous Pandas **describe** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Pandas version here!!!\n",
    "df.toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printSchema\n",
    "The schema is a very import characteristic of a Spark DataFrame.  It tells us what's possible in terms of transformation.  Also, it's the reason DataFrames are so fast since they are typed to a set number of types that are serialized and optimized in Java/Scala behind the scenes.\n",
    "\n",
    "><i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i> The \"schema\" that we've been so excited to see is finally here to explore.  Feel free to take a screenshot and show your friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count\n",
    "Count with caveat:  This will return the count of all rows, including _non-NaN_ values.  Pandas will omit these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic stats in spark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cov(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the data is loaded into an instance's memory -- use for small datasets\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show limited results\n",
    "\n",
    "With Pandas we're used to the `df.head()` as a first step in exploring a dataset.  With Spark this isn't exactly the same.  You need to use the `df.show()` operation in order to explore data as a first step.  Where Pandas formats its DataFrame output for display in nice HTML tables with sensible defaults for output, you have to be a bit more specific about what you're looking at with `show()` when using Spark.\n",
    "\n",
    "\n",
    "> The parameter `truncate` is helpful for truncating attributes for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, truncate=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(19).show()\n",
    "# `show()` can also be chained to certain outputs like `limit`.\n",
    "# `show` by itself is a compound operation for displaying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i> Why should you need to be careful when displaying data in Spark?\n",
    "\n",
    "Hopefully you can see why Pandas is so nice to use for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More DataFrame and Series operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from list of Row objects to list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[row.asDict() for row in df.take(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting DataFrame Series\n",
    "\n",
    "Selecting variables with Spark **DataFrames API** works very similarly to Pandas DataFrames.  When selecting variables in Pandas use a `list` object, passed to a DataFrame object via `[]` brackets like so:\n",
    "\n",
    ">```df[['col1', 'col2']]```\n",
    "\n",
    "The equivalent in spark is using the `.select()` method which takes flat parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select all features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select specific features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"word\", \"pos_score\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Operations\n",
    "\n",
    "With Pandas, you can easily create a new series that's the sum of every row in **\"col1\"** and **\"col2\"** with\n",
    "\n",
    "> `df['col1'] + df['col2']`\n",
    "\n",
    "In Spark, we have to do this through the select function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df[\"pos_score\"] + df[\"neg_score\"]).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do math operations in series as well\n",
    "df.select(df[\"pos_score\"] + 10).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As an \"alias\"\n",
    "As selections are keyed by conditions, they can become hard to read.  We can use an \"alias\" to abstract any selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select((df[\"pos_score\"] + df[\"neg_score\"]).alias(\"total_score\")).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df['pos_score'] + df['neg_score'])\n",
    "df.withColumn(\"new_column\", df['pos_score'] + df['neg_score']).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i class=\"fa fa-question-circle\"></i> Have we changed the original DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data\n",
    "In Pandas we use \"masks\" through dataframe object brackets in order to filter data.\n",
    ">`df[df['feature'] > 0]`\n",
    "\n",
    "In Spark, we use the `filter()` method to select different aspects of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"pos_score\"] > .5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple conditions\n",
    "SAME AS PANDAS!  Thankfully, we don't have to leave our comfort zone with too many oddities here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df[\"pos_score\"] > .5) & (df[\"neg_score\"] > 0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .filter(df[\"pos_score\"] > .5) \\\n",
    "    .filter(df[\"neg_score\"] > 0).show(5)  # Filters can be chained per line using the \\ newline escape sequence character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter as an expression\n",
    "Pandas has a similar function called \"where\".  However, with Spark `filter`, we can filter by shorthand expressions when referencing column sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"pos_score > .5\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = \"\"\"\n",
    "pos_score > .5 AND \n",
    "neg_score > 0 \n",
    "\"\"\"\n",
    "\n",
    "df.filter(condition).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.neg_score.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.pos_score.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"pos_score > .5 AND pos_score < 1.0 AND neg_score > 0\") \\\n",
    "    .sort(df.pos_score.desc(), df.neg_score.desc()) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"pos_score > .5 AND pos_score < 1.0 AND neg_score > 0\") \\\n",
    "    .sort(df.pos_score.asc(), df.neg_score.desc()) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AS SQL!?\n",
    "\n",
    "\n",
    "Working with DataFrames and SQL is as easy as creating a **temporary view**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = spark.sql(\"SELECT * FROM sentiment LIMIT 100\")\n",
    "sentiment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.toPandas()['neg_score'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "sentiment.toPandas()['neg_score'].hist()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Views select DataFrames\n",
    "So the same transformations can be applied to DataFrames as we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out another dataset using Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load up the \"Pokemon\" basic Pokedex dataset\n",
    "First try without inferring the schema and without the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "sqlContext = ps.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(\n",
    "    path=\"./data/pokedex_basic.csv\",\n",
    "    header=True,\n",
    "    # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    # Not always perfect but works well in most cases as of 2.1+\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show(truncate=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check out the dataset with infer schema parameter but without header.\n",
    "How does it work with / without?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Create a tempory view with the Pokedex DataFrame called \"pokemon\"\n",
    "Then \n",
    "```sql SELECT * FROM pokemon LIMIT 10```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df.createOrReplaceTempView(\"pokemon\")\n",
    "sqlContext.sql(\"SELECT * FROM pokemon LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a Which is the strongest Pokemon by `Type`?\n",
    "Using Spark DataFrame operations.  Research Spark's \"groupBy\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_df = df.select('Name', 'Type', 'attack').groupBy(\n",
    "    'Type').max().sort('max(attack)', ascending=False)\n",
    "count_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = count_df.withColumn('attack', count_df['max(attack)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.join(df, on=['Type', 'attack']).select(\n",
    "    'Name', 'Type', 'attack').sort('attack', ascending=False).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b Which is the strongest Pokemon by Type?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "sql = \"\"\"\n",
    "SELECT p.Name, tbl.Type, tbl.TotalMax\n",
    "FROM (\n",
    "    SELECT Type, MAX(attack) AS TotalMax\n",
    "    FROM pokemon\n",
    "    GROUP BY Type\n",
    ") AS tbl \n",
    "LEFT OUTER JOIN pokemon p \n",
    "ON tbl.Type = p.Type AND tbl.TotalMax = p.attack\n",
    "ORDER BY TotalMax DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "strongest_pokemon = sqlContext.sql(sql)\n",
    "strongest_pokemon.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a Which Pokemon has the best combined Attack and Defence?\n",
    "Using Spark DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df.withColumn(\"Poketotal\", df[\"Attack\"] + df[\"Defense\"]\n",
    "              ).sort(\"Poketotal\", ascending=0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b Which Pokemon has the best combined Attack and Defence?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "sql = \"\"\"\n",
    "SELECT p.*, p.Attack + p.Defense AS Poketotal\n",
    "FROM pokemon p\n",
    "ORDER BY Poketotal DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "sqlContext.sql(sql).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create a new feature called \"Pokevalue\" that is the combined Attack, Defense and scaled by .2 of the Pokemon HP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "sql = \"\"\"\n",
    "SELECT p.*, (p.Attack + p.Defense) * (p.HP * .2) AS Pokevalue\n",
    "FROM pokemon p\n",
    "ORDER BY Pokevalue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "sqlContext.sql(sql).show(truncate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Spark is one of the essential big data tools. While we use it only on our laptops for now, spark jobs can easily be run on a cluster. We will practice the tools learned in this lesson more and are going to see how to use the machine learning library of spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "579px",
    "left": "561.111px",
    "right": "20px",
    "top": "120px",
    "width": "337px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
