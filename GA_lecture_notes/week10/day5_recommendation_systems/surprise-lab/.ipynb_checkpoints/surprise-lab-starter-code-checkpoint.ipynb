{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "\n",
    "# Recommendations with surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Use-the-built-in-movielens-100k-dataset\" data-toc-modified-id=\"Use-the-built-in-movielens-100k-dataset-1\">Use the built-in movielens-100k dataset</a></span></li><li><span><a href=\"#Load-the-movielens-100k-dataset-from-disk\" data-toc-modified-id=\"Load-the-movielens-100k-dataset-from-disk-2\">Load the movielens-100k dataset from disk</a></span><ul class=\"toc-item\"><li><span><a href=\"#Instantiate-the-algorithm\" data-toc-modified-id=\"Instantiate-the-algorithm-2.1\">Instantiate the algorithm</a></span></li><li><span><a href=\"#Extract-the-model-parameters\" data-toc-modified-id=\"Extract-the-model-parameters-2.2\">Extract the model parameters</a></span></li><li><span><a href=\"#Evaluate-the-model:\" data-toc-modified-id=\"Evaluate-the-model:-2.3\">Evaluate the model:</a></span></li><li><span><a href=\"#Put-the-predictions-in-a-dataframe\" data-toc-modified-id=\"Put-the-predictions-in-a-dataframe-2.4\">Put the predictions in a dataframe</a></span></li><li><span><a href=\"#Correlations-between-predicted-and-true-ratings\" data-toc-modified-id=\"Correlations-between-predicted-and-true-ratings-2.5\">Correlations between predicted and true ratings</a></span></li></ul></li><li><span><a href=\"#Cross-validation,-train-test-split-and-grid-search\" data-toc-modified-id=\"Cross-validation,-train-test-split-and-grid-search-3\">Cross validation, train-test split and grid search</a></span></li><li><span><a href=\"#Slope-One\" data-toc-modified-id=\"Slope-One-4\">Slope One</a></span></li><li><span><a href=\"#KNN-with-Means\" data-toc-modified-id=\"KNN-with-Means-5\">KNN with Means</a></span></li><li><span><a href=\"#Precision@k-and-Recall@k\" data-toc-modified-id=\"Precision@k-and-Recall@k-6\">Precision@k and Recall@k</a></span></li><li><span><a href=\"#Top-n-predictions\" data-toc-modified-id=\"Top-n-predictions-7\">Top-n predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Coverage\" data-toc-modified-id=\"Coverage-7.1\">Coverage</a></span></li><li><span><a href=\"#Novelty\" data-toc-modified-id=\"Novelty-7.2\">Novelty</a></span></li><li><span><a href=\"#Evaluate-the-similarity-of-the-top-k-predictions-between-all-pairs-of-users\" data-toc-modified-id=\"Evaluate-the-similarity-of-the-top-k-predictions-between-all-pairs-of-users-7.3\">Evaluate the similarity of the top-k predictions between all pairs of users</a></span></li><li><span><a href=\"#Content-data\" data-toc-modified-id=\"Content-data-7.4\">Content data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will make use of the [surprise package](https://surprise.readthedocs.io/en/stable/index.html), a package dedicated to recommendation systems.\n",
    "\n",
    "`conda install -c conda-forge scikit-surprise`\n",
    "\n",
    "First we will need some data. Load the built-in dataset. It will have to be downloaded first.\n",
    "It is a very famous dataset about movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from surprise import Dataset\n",
    "# Load the movielens-100k dataset (download it if needed),\n",
    "#data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load surprise\n",
    "import surprise as sur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the built-in movielens-100k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ml-100k could not be found. Do you want to download it? [Y/n] "
     ]
    }
   ],
   "source": [
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = sur.Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the famous SVD algorithm.\n",
    "algo = sur.SVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 5-fold cross-validation and print results\n",
    "sur.model_selection.cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the movielens-100k dataset from disk\n",
    "\n",
    "With the above command we could load the data in a simplified and already prepared way. As reading and preparing other files is not that straight-forward, we will rather load the file from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\n",
    "    '~/.surprise_data/ml-100k/ml-100k/u.data', sep='\\t', header=None)\n",
    "df_data.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.rating.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader function serves to specify which columns are referring to user, items and ratings as well as the rating scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = sur.Reader(rating_scale=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data_1 = sur.Dataset.load_from_df(\n",
    "    df_data[['user_id', 'item_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = sur.SVD(random_state=1,\n",
    "               biased=True,  # isolate biases\n",
    "               reg_all=0.2,  # use regularisation (the same for all)\n",
    "               n_epochs=20,  # number of epochs for stochastic gradient descent search\n",
    "               n_factors=100  # number of factors to retain in SVD\n",
    "               )\n",
    "\n",
    "# we have to build a training set from the data\n",
    "trainset_full = data_1.build_full_trainset()\n",
    "# fit the model\n",
    "algo.fit(trainset_full)\n",
    "\n",
    "# we prepare a test set from the training set\n",
    "trainsetfull_build = trainset_full.build_testset()\n",
    "# obtain the predictions\n",
    "predictions_full = algo.test(trainsetfull_build)\n",
    "# evaluate the predictions\n",
    "print(sur.accuracy.rmse(predictions_full, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = algo.default_prediction()\n",
    "bu = algo.bu\n",
    "bi = algo.bi\n",
    "pu = algo.pu\n",
    "qi = algo.qi\n",
    "puqi = pu.dot(qi.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that internally surprise uses other (inner) indices for users and items than in the original data.\n",
    "> The original ones are the raw indices. There are functions to translate between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we can reconstruct the predictions using the parameters\n",
    "i = 10\n",
    "print(predictions_full[i])\n",
    "print()\n",
    "uid = predictions_full[i].uid\n",
    "iid = predictions_full[i].iid\n",
    "u_inner = trainset_full.to_inner_uid(uid)\n",
    "i_inner = trainset_full.to_inner_iid(iid)\n",
    "\n",
    "pred_calc = mu + bu[u_inner] + bi[i_inner] + puqi[u_inner, i_inner]\n",
    "print('Results agree:', predictions_full[i].est - pred_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sur.accuracy.rmse(predictions_full)\n",
    "sur.accuracy.mae(predictions_full);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the predictions in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame([(x.r_ui, x.est) for x in predictions_full],\n",
    "                       columns=['Rating', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct RMSE\n",
    "np.sqrt(df_pred.apply(lambda x: (x[0]-x[1])**2, axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct MAE\n",
    "df_pred.apply(lambda x: abs(x[0]-x[1]), axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between predicted and true ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.corr(method='kendall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation, train-test split and grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from https://surprise.readthedocs.io/en/stable/FAQ.html?highlight=raw_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "raw_ratings = data_1.raw_ratings\n",
    "np.random.seed(1)\n",
    "# shuffle ratings if you want\n",
    "random.shuffle(raw_ratings)\n",
    "\n",
    "# A = 90% of the data, B = 10% of the data\n",
    "threshold = int(.9 * len(raw_ratings))\n",
    "A_raw_ratings = raw_ratings[:threshold]\n",
    "B_raw_ratings = raw_ratings[threshold:]\n",
    "\n",
    "print(len(A_raw_ratings))\n",
    "print(len(B_raw_ratings))\n",
    "\n",
    "data_1.raw_ratings = A_raw_ratings  # data is now the set A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_1.raw_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = sur.SVD(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = sur.model_selection.cross_validate(\n",
    "    algo, data_1, measures=['RMSE', 'MAE'], cv=5)\n",
    "pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your best algo with grid search.\n",
    "print('Grid Search...')\n",
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005]}\n",
    "grid_search = sur.model_selection.GridSearchCV(sur.SVD,\n",
    "                                               param_grid,\n",
    "                                               measures=['rmse'],\n",
    "                                               cv=3,\n",
    "                                               refit=True)\n",
    "grid_search.fit(data_1)\n",
    "\n",
    "algo = grid_search.best_estimator['rmse']\n",
    "\n",
    "# retrain on the whole set A\n",
    "trainset = data_1.build_full_trainset()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Compute score on training set\n",
    "trainset_build = trainset.build_testset()\n",
    "predictions_train = algo.test(trainset_build)\n",
    "print('Training score ', end='   ')\n",
    "sur.accuracy.rmse(predictions_train)\n",
    "\n",
    "# Compute score on rated test set\n",
    "testset = data_1.construct_testset(B_raw_ratings)  # testset is now the set B\n",
    "predictions_test = algo.test(testset)\n",
    "print('Test score (rated items) ', end=' ')\n",
    "sur.accuracy.rmse(predictions_test)\n",
    "\n",
    "# Compute score on unrated data\n",
    "# The anti-test set is the part where we did not have any ratings\n",
    "no_ratings = trainset.build_anti_testset()\n",
    "predictions_no_ratings = algo.test(no_ratings)\n",
    "print('Test score (unrated items) ', end='   ')\n",
    "sur.accuracy.rmse(predictions_no_ratings, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainset_build), len(testset), len(no_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_train[0])\n",
    "print(predictions_test[0])\n",
    "print(predictions_no_ratings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract model parameters\n",
    "mu = algo.default_prediction()\n",
    "print(f'Training set mean: {mu:.6}')\n",
    "bu = algo.bu\n",
    "bi = algo.bi\n",
    "pu = algo.pu\n",
    "qi = algo.qi\n",
    "puqi = pu.dot(qi.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct predictions\n",
    "i = 10\n",
    "print(predictions_train[i])\n",
    "print()\n",
    "uid = predictions_train[i].uid\n",
    "iid = predictions_train[i].iid\n",
    "u_inner = trainset.to_inner_uid(uid)\n",
    "i_inner = trainset.to_inner_iid(iid)\n",
    "\n",
    "pred_calc = mu + bu[u_inner] + bi[i_inner] + puqi[u_inner, i_inner]\n",
    "print('Results agree:', predictions_train[i].est - pred_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope One\n",
    "\n",
    "Repeat the same steps with the slope one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = sur.SlopeOne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with Means\n",
    "\n",
    "Repeat the same steps with the kNN with means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = sur.KNNWithMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision@k and Recall@k\n",
    "\n",
    "Obtain  precision@k and recall@k following the [example](https://surprise.readthedocs.io/en/stable/FAQ.html#how-to-compute-precision-k-and-recall-k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-n predictions\n",
    "\n",
    "Obtain the n top-ranked predictions for each user following the [example](https://surprise.readthedocs.io/en/stable/FAQ.html#how-to-get-the-top-n-recommendations-for-each-user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage\n",
    "\n",
    "Calculate the coverage of the top-ranked recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novelty\n",
    "\n",
    "Calculate the novelty of the top-ranked recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the similarity of the top-k predictions between all pairs of users\n",
    "\n",
    "Form a user-item matrix with ones indicating the top movies recommended to each user.\n",
    "Use scipy's `pdist` function to calculate the similarities of all pairs of rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content data\n",
    "\n",
    "Now work with the further data files containing content information. They can be found in \n",
    "\n",
    "`.surprise_data/ml-100k/ml-100k/u.item`\n",
    "\n",
    "`.surprise_data/ml-100k/ml-100k/u.user`\n",
    "\n",
    "Take the movie data into account to evaluate the similarity of the recommended films regarding genre. \n",
    "\n",
    "\n",
    "Translate the recommended movie ids into movie titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv(\n",
    "    '/Users/crahmede/.surprise_data/ml-100k/ml-100k/u.user', sep='|', header=None)\n",
    "df_users.columns = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = pd.read_csv('/Users/crahmede/.surprise_data/ml-100k/ml-100k/u.item',\n",
    "                       sep='|', header=None, encoding='latin')\n",
    "df_items.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date',\n",
    "                    'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                    'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                    'FilmNoir', 'Horror', 'Musical', 'Mystery', 'Romance', 'SciFi',\n",
    "                    'Thriller', 'War', 'Western']\n",
    "df_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
