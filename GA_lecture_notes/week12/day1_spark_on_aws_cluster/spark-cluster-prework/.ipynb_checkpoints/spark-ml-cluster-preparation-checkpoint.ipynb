{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"> \n",
    "# Spark Machine Learning Cluster Preparations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the data and run a grid search. Then we save our model and the best configuration and scores. Since our aim is going to be to run the entire process on a cluster, the latter steps are useful for retrieving our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps   \n",
    "import warnings         \n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = ps.sql.SparkSession(sc)\n",
    "\n",
    "spark_df = spark.read.csv(\n",
    "    path='data/boston_housing.csv', \n",
    "    header=True,\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    inferSchema=True,\n",
    "    enforceSchema=False\n",
    "    )\n",
    "\n",
    "(data_train, data_test) = spark_df.randomSplit([0.7, 0.3], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in spark_df.columns if col != 'MEDV']\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=features,\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "scaler = StandardScaler(withMean=True,\n",
    "                        inputCol=\"features\",\n",
    "                        outputCol=\"scaledfeatures\")\n",
    "\n",
    "model = LinearRegression(featuresCol=scaler.getOutputCol(),\n",
    "                         labelCol='MEDV',\n",
    "                         maxIter=3000,\n",
    "                         regParam=0.0,\n",
    "                         elasticNetParam=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, scaler, model])\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction',\n",
    "                                labelCol='MEDV',\n",
    "                                metricName='r2')\n",
    "\n",
    "reg_strengths = sc.range(-4, 4).map(lambda x: 10**x).collect()\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.regParam, reg_strengths) \\\n",
    "    .addGrid(model.fitIntercept, [True, False]) \\\n",
    "    .build()\n",
    "\n",
    "# the actual gridsearch\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5,\n",
    "                          parallelism=2)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "model_fit = crossval.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310936297006648\n",
      "0.6934885107911399\n",
      "0.7218117084843778\n"
     ]
    }
   ],
   "source": [
    "predictions_train = model_fit.transform(data_train)\n",
    "predictions_test = model_fit.transform(data_test)\n",
    "predictions_all = model_fit.transform(spark_df)\n",
    "\n",
    "print(evaluator.evaluate(predictions_train))\n",
    "print(evaluator.evaluate(predictions_test))\n",
    "print(evaluator.evaluate(predictions_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regParam': 1.0, 'fitIntercept': True}\n",
      "\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0, current: 0.0)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. (default: 1.35)\n",
      "featuresCol: features column name (default: features, current: scaledfeatures)\n",
      "fitIntercept: whether to fit an intercept term (default: true, current: true)\n",
      "labelCol: label column name (default: label, current: MEDV)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError) (default: squaredError)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 3000)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0, current: 1.0)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto) (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n"
     ]
    }
   ],
   "source": [
    "java_model = model_fit.bestModel.stages[2]._java_obj\n",
    "best_parameters = {param.name: java_model.getOrDefault(java_model.getParam(param.name))\n",
    "       for param in paramGrid[0]}\n",
    "\n",
    "print(best_parameters)\n",
    "print()\n",
    "print(java_model.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best model\n",
    "\n",
    "Spending a lot of time fitting the model is not much worth if we don't save the model, even more so if we work with an expensive cluster from which we have to retrieve our results.\n",
    "\n",
    "The model will be saved in a directory for which we generate a name using the current time.\n",
    "To verify that everything worked we load the saved model again. Additionally, we write the scores and model parameters to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time_now = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "path = 'model-{}'.format(time_now)\n",
    "model_fit.bestModel.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310936297006648\n",
      "0.6934885107911399\n",
      "0.7218117084843778\n"
     ]
    }
   ],
   "source": [
    "best_model = PipelineModel.load(path)\n",
    "\n",
    "l_predictions_train = best_model.transform(data_train)\n",
    "l_predictions_test = best_model.transform(data_test)\n",
    "l_predictions_all = best_model.transform(spark_df)\n",
    "\n",
    "print(evaluator.evaluate(l_predictions_train))\n",
    "print(evaluator.evaluate(l_predictions_test))\n",
    "print(evaluator.evaluate(l_predictions_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_line(text, file, n_lines=1):\n",
    "    return file.write(str(text)+'\\n'*n_lines)\n",
    "\n",
    "with open('results-{}.txt'.format(time_now), 'w') as file:\n",
    "    add_line(best_parameters, file, n_lines=2)\n",
    "    add_line(java_model.explainParams(), file, n_lines=2)\n",
    "    add_line(evaluator.evaluate(predictions_train), file)\n",
    "    add_line(evaluator.evaluate(predictions_test), file)\n",
    "    add_line(evaluator.evaluate(predictions_all), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the file [ml_load_model](ml_load_model.ipynb) you will find the code for loading the saved model and evaluating it on the data.\n",
    "\n",
    "On the cluster, we don't want to go through the lengthy process of setting up anaconda with notebook access. Rather we are going to simply submit a script based on the code above which will load the data and perform the computation. You will find it [here](scripts/spark_ml_cluster.py), and the same can be done for the model loading procedure with another [script](scripts/spark_model_loader.py).\n",
    "\n",
    "Each of them can be run from the command line. Change into the scripts directory, activate your pyspark environment, and the run the following:\n",
    "\n",
    "```bash\n",
    "spark-submit spark_ml_cluster.py\n",
    "```\n",
    "\n",
    "Once it has finished, it will have stored a folder `model-...` and you can load it replacing the appropriate folder name using\n",
    "\n",
    "```bash\n",
    "spark-submit spark_model_loader.py model-...\n",
    "```\n",
    "\n",
    "The print outs are buried among a lot of verbosity. To save them redirect the output into a file:\n",
    "\n",
    "```bash\n",
    "spark-submit spark_model_loader.py model-... > result.txt\n",
    "```\n",
    "\n",
    "Note that once running the scripts on a cluster, you will have to use\n",
    "\n",
    "```python\n",
    "sc = ps.SparkContext('yarn')\n",
    "```\n",
    "\n",
    "when creating the spark context. Also you should adjust the parallelism level in the grid search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
