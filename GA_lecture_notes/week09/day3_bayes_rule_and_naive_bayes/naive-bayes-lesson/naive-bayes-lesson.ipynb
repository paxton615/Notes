{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Describe Naive Bayes\n",
    "- Choose a Naive Bayes implementation based on your use case\n",
    "- Implement a Naive Bayes model through scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1\">Learning Objectives</a></span></li><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2\">Introduction</a></span></li><li><span><a href=\"#A-Simple-Spam-Example\" data-toc-modified-id=\"A-Simple-Spam-Example-3\">A Simple Spam Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem:-Multiple-Features-Require-Joint-Probabilities\" data-toc-modified-id=\"Problem:-Multiple-Features-Require-Joint-Probabilities-3.1\">Problem: Multiple Features Require Joint Probabilities</a></span></li><li><span><a href=\"#Solution:-The-Naive-Bayes-Independence-Assumption\" data-toc-modified-id=\"Solution:-The-Naive-Bayes-Independence-Assumption-3.2\">Solution: The Naive Bayes Independence Assumption</a></span></li><li><span><a href=\"#Spam-Application:-Multiple-Features\" data-toc-modified-id=\"Spam-Application:-Multiple-Features-3.3\">Spam Application: Multiple Features</a></span></li></ul></li><li><span><a href=\"#Production-Issues\" data-toc-modified-id=\"Production-Issues-4\">Production Issues</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5\">Summary</a></span></li><li><span><a href=\"#Implementation-in-Scikit-Learn\" data-toc-modified-id=\"Implementation-in-Scikit-Learn-6\">Implementation in Scikit-Learn</a></span></li><li><span><a href=\"#Discriminative-and-Generative-Models\" data-toc-modified-id=\"Discriminative-and-Generative-Models-7\">Discriminative and Generative Models</a></span></li><li><span><a href=\"#Additional-Resources\" data-toc-modified-id=\"Additional-Resources-8\">Additional Resources</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Naive Bayes is a classification algorithm relying on Bayes' rule to assign class labels $y$ for given feature values $X$. As in all other classification models, one is interested in determining the probability of having a certain class label when a certain combination of feature values is obtained\n",
    "\n",
    "$$P(y|X)$$\n",
    "\n",
    "\n",
    "\n",
    "Using Bayes' rule we can write this as\n",
    "\n",
    "$$\n",
    "P(y|X) = \n",
    "\\frac{P(X|y)P(y)}\n",
    "{P(X)}\n",
    "$$\n",
    "\n",
    "which is saying that if we know how likely a certain combination of feature values is within a particular class, if we know how likely that class is to occur, and if we know how likely that combination of feature values is across all classes, we also know how likely the class is given the feature values.\n",
    "\n",
    "That is all we need! If we have enough data with examples for all possible feature values, we just have to look up the respective frequencies in the data and we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Simple Spam Example\n",
    "\n",
    "Suppose we are trying to predict spam emails. For now, we have one feature: whether or not the email mentions \"guarantee.\" All we need to do is to look up what is the fraction of emails containing this word, the fraction of spam emails, and the fraction of spam emails which do contain that word.\n",
    "\n",
    "$G$ = Guarantee, $S$ = Is spam.\n",
    "\n",
    " $$P\\left(S|G\\right) = \\frac{P\\left(G|S\\right)P\\left(S\\right)}{P(G)} = \\frac{P\\left(G|S\\right)P\\left(S\\right)}{P(G|S)P(S) + P(G|\\neg{S})P(\\neg{S})}$$\n",
    "\n",
    "\n",
    "The denominator looks complicated, but it actually isn't. Because $G$ is binary (either present or not), then:\n",
    "\n",
    "> $$P(G) = P(G,S) + P(G,\\neg{S})=P(G|S)P(S) + P(G|\\neg{S})P(\\neg{S})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Multiple Features Require Joint Probabilities\n",
    "\n",
    "In this spam example, we only had one feature: $G$. But, in general, we'll probably use more than one. Really, we want to see some feature vector $X_1, X_2, ..., X_n$ (standing for all the words contained in the email):\n",
    "\n",
    "$$P\\left(S|X_1,\\ldots, X_n\\right) = \\frac{P\\left(X_1,\\ldots, X_n|S\\right)P(S)}{P(X_1,  \\ldots, X_n|S)P(S) + P(X_1, \\ldots, X_n|\\neg{S})P(\\neg S)}$$\n",
    "\n",
    "For example, what is the likelihood that an email is spam given that the email mentions \"guarantee,\" \"oil,\" \"prince,\" and \"million,\" but not \"meeting\", \"colleague,\" and \"dad?\"\n",
    "\n",
    "With a lot of features, calculating the joint probabilities quickly becomes complicated. We would definitely need a lot of data to ensure we have enough feature combinations. \n",
    "If we were interested in the presence or absence of only a 100 selected words, we would have $2^{100}\\approx 10^{30}$ possible feature combinations, and we need to estimate the probability for each of them occurring. The number of possible combinations grows exponentially with the number of features, and we are even just talking about presence or absence.\n",
    "\n",
    "No matter how diligent we are, we may never collect a single training example that contains the precise combination of feature words we need. Therefore, we would be unable to classify a new email containing a particular combination of words.\n",
    "\n",
    "Although each probability is easy to calculate, the joint probability model is simply not practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution: The Naive Bayes Independence Assumption\n",
    "\n",
    "We're stuck, because conditional joint probabilities are required for multiple features. This means there are exponentially more probabilities to compute and data to collect.\n",
    "\n",
    "To get around this, let's make an assumption: \n",
    "\n",
    "> **All $X_i$ are conditionally independent given $S$** \n",
    "\n",
    "(where $S$ indicates \"is spam\"). This means that, given $S$, no two $X_i$ depend on each other. For example, the words \"million\" and \"prince\" each independently indicate that an email is spam. So, it's not the complex interaction of words that determines spam; each feature can indicate whether or not an email is spam independently.\n",
    "\n",
    "\n",
    "Recall that if events $A$ and $B$ are independent, then the joint probability is simply \n",
    "\n",
    "$$P(A,B) = P(A)P(B)$$ \n",
    "\n",
    "Similarly, if $A$ and $B$ are conditionally independent of $S$, then \n",
    "\n",
    "$$P(A,B|S) = P(A|S)P(B|S)$$\n",
    "\n",
    "This formula works out well in general, too:\n",
    "\n",
    "$$P\\left(X_{1}X_{2} \\dots X_{n}|S\\right) = P\\left(X_{1} |S\\right)  P\\left(X_{2} |S\\right) \\cdots P\\left(X_{n} |S\\right)$$\n",
    "\n",
    "> **Caution:** Of course, this assumption is rarely (if ever) true. Often, it requires precise reading to tell whether or not an email was written by a native speaker. In this case, it's often not the particular words used, but how they are used in context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we were considering how indicative the words \"guarantee\" and \"million\" are for being spam, we would write the following\n",
    "\n",
    "$G$ = Guarantee, $M$ = Million, $S$ = Is spam:\n",
    "\n",
    "$$P(S,G,M) = P(G,M|S)P(S) = P(G|S)P(M|S)P(S)$$\n",
    "\n",
    "\n",
    "> None of these probabilities require us to examine multiple features at once in our data set, making them drastically easier to compute. \n",
    "\n",
    "In reality, model parameters/coefficients are unlikely to be independent. But Naive Bayes makes exactly this assumption and often works well despite this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Spam Application: Multiple Features\n",
    "\n",
    "How is this used in practice? Let's combine the Naive Bayes simplification above with our original formula (the denominator is computed the same way as before, combined with our naive assumption):\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "P\\left(S|G,M\\right) &=& \\frac{P(S,G,M)}{P(G,M)} \\\\\n",
    " &=& \\frac{P\\left(G,M|S\\right)P\\left(S\\right)}{P(G,M|S)P(S) + P(G,M|\\neg{S})P(\\neg{S})} \\\\\n",
    " &=& \\frac{P\\left(G|S\\right)P\\left(M|S\\right)P\\left(S\\right)}{P(G|S)P(M|S)P(S) + P(G|\\neg{S})P(M|\\neg{S})P(\\neg{S})}\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    " \n",
    "Typically, we compute this probability for each class (in this case, just $S$ or $\\neg S$), then predict the class with the highest probability. Note that, for all of these, the denominator $P(G,M)$ is constant. Hence, this formula is often written as \"proportional\" ($\\propto$), considerably simplifying it. Instead of comparing the exact probabilities, we can simply see how they score relative to each other. So:\n",
    "\n",
    " $$P\\left(S|GM\\right) \\propto P\\left(G|S\\right)P\\left(M|S\\right)P\\left(S\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Production Issues\n",
    "\n",
    "Accidentally assuming a zero probability for any word appearing in a given class could present major problems — the entire probability estimation would be zero.\n",
    "\n",
    "- **New features:** What if a particular feature was never seen in our training data? Instead of using a zero probability, we should use a technique such as [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to estimate a small non-zero probability.\n",
    "\n",
    "- **Underflow:** Probabilities could be very small if some features rarely occur in some classes. Recall that floating point often gives us trouble because of its limited precision — small floats tend toward zero. We can approach this problem by storing the logarithm of each probability, $P_i$, instead of $P_i$ itself:\n",
    "\n",
    "$$\\log(P_1P_2) = \\log\\ P_1 + \\log\\ P_2$$\n",
    "\n",
    "$$e^{\\log\\ P_1} = P_1$$\n",
    "\n",
    "So: $$P_1P_2 \\cdots P_n = e^{\\log\\ P_1 + \\ldots + \\log\\ P_n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "**Why is the Naive Bayes formula important?** With the independence assumption, we don't need to compute every joint probability distribution, we only need to compute the probability of each word separately: $P(G|S)$ and $P(M|S)$. \n",
    "\n",
    "These calculations can be performed quickly using our training data. The downside is that if spam is actually determined by some complex interaction between \"guarantee\" and \"millions\" (e.g., one of the words on its own is not a good sign of spam, but both in the same email is), then the independence assumption does not hold and our model will not have the capacity to predict spam correctly.\n",
    "\n",
    "To make Naive Bayes a classifier, all we have to do is compute the probability of $P(y|X)$ for each class, $y$. In math notation, this is:\n",
    "\n",
    "$$ P(y \\;|\\; X_1, \\ldots, X_n) \\propto P(y) \\prod_{i=1}^n P(X_i \\;|\\; y) \\\\\n",
    "\\downarrow \\\\\n",
    "\\hat{y} = {\\rm arg} \\; \\underset{y}{\\rm max} \\; P(y) \\prod_{i=1}^n P(X_i \\;|\\; y)$$\n",
    "\n",
    "> The last expression simply means that we predict the class label with the highest predicted probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation in Scikit-Learn\n",
    "\n",
    "- [Docs Bernoulli NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "- [Docs Multinomial NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "- [Docs Gaussian NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "<img src=\"./images/naive-bayes.png\">\n",
    "\n",
    "The differences can be summarized as:\n",
    "-    **BernoulliNB** is designed for binary/Boolean features.\n",
    "-    The **Multinomial Naive Bayes classifier** is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as `tf-idf` may also work.\n",
    "-    **GaussianNB** is designed for continuous, normally distributed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paxton615/GA/DSI9-lessons/week09/day3_bayes_rule_and_naive_bayes/naive-bayes-lesson'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_spam</th>\n",
       "      <th>getzed</th>\n",
       "      <th>86021</th>\n",
       "      <th>babies</th>\n",
       "      <th>sunoco</th>\n",
       "      <th>ultimately</th>\n",
       "      <th>thk</th>\n",
       "      <th>voted</th>\n",
       "      <th>spatula</th>\n",
       "      <th>fiend</th>\n",
       "      <th>...</th>\n",
       "      <th>itna</th>\n",
       "      <th>borin</th>\n",
       "      <th>thoughts</th>\n",
       "      <th>iccha</th>\n",
       "      <th>videochat</th>\n",
       "      <th>freefone</th>\n",
       "      <th>pist</th>\n",
       "      <th>reformat</th>\n",
       "      <th>strict</th>\n",
       "      <th>69698</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_spam  getzed  86021  babies  sunoco  ultimately  thk  voted  spatula  \\\n",
       "0        0       0      0       0       0           0    0      0        0   \n",
       "1        0       0      0       0       0           0    0      0        0   \n",
       "2        1       0      0       0       0           0    0      0        0   \n",
       "3        0       0      0       0       0           0    0      0        0   \n",
       "4        0       0      0       0       0           0    0      0        0   \n",
       "\n",
       "   fiend  ...  itna  borin  thoughts  iccha  videochat  freefone  pist  \\\n",
       "0      0  ...     0      0         0      0          0         0     0   \n",
       "1      0  ...     0      0         0      0          0         0     0   \n",
       "2      0  ...     0      0         0      0          0         0     0   \n",
       "3      0  ...     0      0         0      0          0         0     0   \n",
       "4      0  ...     0      0         0      0          0         0     0   \n",
       "\n",
       "   reformat  strict  69698  \n",
       "0         0       0      0  \n",
       "1         0       0      0  \n",
       "2         0       0      0  \n",
       "3         0       0      0  \n",
       "4         0       0      0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../../../resource-datasets/spam/spam_words_wide.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Which scikit-learn Naive Bayes implementation should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "target = data.pop('is_spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=None, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = naive_bayes.BernoulliNB(binarize=None)\n",
    "model.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9373670608883271\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(model, data, target, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Check**: Is that good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8659368269921034"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-target.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative and Generative Models\n",
    "\n",
    "- Naive Bayes is often called a Generative Model whereas Logistic Regression is a discriminative model.\n",
    "- In a discriminative model we try to model $P(y|X)$ directly, we are only interested in how likely the class is given any combination of feature values. That is sufficient to obtain the decision boundary in feature space where both classes are equally likely.\n",
    "- In a generative model, we obtain full access to the joint distribution of $P(y,X)$. From this joint distribution we could then sample to obtain new possibly observable class and feature combinations in the appropriate proportions learned from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [An interesting slide](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf) from a Stanford MOOC that has a section on Naive Bayes\n",
    "- [A much more technical paper](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf) comparing Naive Bayes to logistic regression\n",
    "- [More exposition on Naive Bayes](http://blog.yhat.com/posts/naive-bayes-in-python.html)\n",
    "- [Naive Bayes from scratch](http://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)\n",
    "- [Mixing different variable types in Naive Bayes](https://sebastianraschka.com/faq/docs/naive-bayes-vartypes.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
