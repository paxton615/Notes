{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](https://snag.gy/uvESGH.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "\n",
    "### Core\n",
    "- Extract features from unstructured text using Scikit Learn\n",
    "    - Count vectorizer\n",
    "    - TFIDF vectorizer\n",
    "- Describe downsides of bag-of-word approaches\n",
    "- Remove stop words\n",
    "\n",
    "\n",
    "### Target\n",
    "- Identify parts of speech using NLTK\n",
    "    - Stemming \n",
    "    - Segmentation\n",
    "    - Parts of speech tagging\n",
    "\n",
    "\n",
    "### Stretch\n",
    "\n",
    "- Describe how TFIDF works and calculate scores by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to text feature extraction](#intro)\n",
    "- [An NLP project: rapstats.io](#rapstats)\n",
    "- [Common NLP problems](#common)\n",
    "- [Common NLP models](#models)\n",
    "- [A simple example](#simple)\n",
    "- [Bag-of-words / word counting](#bow)\n",
    "    - [Sklearn `CountVectorizer`](#countvectorizer)\n",
    "- [Term frequency - inverse document frequency](#tfidf)\n",
    "    - [Sklearn `TfidfVectorizer`](#tfidf-vec)\n",
    "- [Downsides to bag-of-words](#downsides-bow)\n",
    "- [Segmentation](#segmentation)\n",
    "    - [NLTK sentencer](#nltk-sentencer)\n",
    "- [Stemming with NLTK](#stem-nltk)\n",
    "    - [Stemming approaches](#group)\n",
    "- [Stop words](#stopwords)\n",
    "- [Part of speech tagging](#pos)\n",
    "- [Unicode: a common pitfall](#unicode)\n",
    "- [Conclusion](#conclusion)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction to text feature extraction\n",
    "\n",
    "---\n",
    "\n",
    "The models we have been using so far accept a 2D matrix of real numbers as input `X` and a target vector of classes or numbers `y`. What if our starting point data is not given in the form of a table of numbers, but rather is unstructured? This is the case when working with text documents.\n",
    "\n",
    "We need a way to go from unstructured data to our numeric `X` matrix in order to use the same models. This is called _feature extraction_ and this lesson is dedicated to it.\n",
    "\n",
    "The applications of using text data in statistical modeling are practically infinite. Some examples include:\n",
    "- Sentiment analysis of Yelp reviews\n",
    "- Identifying topics of news articles\n",
    "- Classification of political authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='rapstats'></a>\n",
    "## An example NLP Project:  rapstats.io\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"http://rapstats.io\"><img src=\"https://snag.gy/8GSVqf.jpg\"></a>\n",
    "\n",
    "<img src=\"https://snag.gy/8eJNFv.jpg\" style=\"width: 300px; float: left;\">\n",
    "<img src=\"https://snag.gy/2Hz0o7.jpg\" style=\"width: 300px;\">\n",
    "\n",
    "**See Also:**\n",
    "\n",
    "- [Largest Vocabulary in Hip Hop](http://poly-graph.co/vocabulary.html)\n",
    "- [Rap Genius: Rapstats](http://genius.com/rapstats)\n",
    "- [Rap Lyric Generator, Hieu Nguyen, Brian Sa](http://nlp.stanford.edu/courses/cs224n/2009/fp/5.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='common'></a>\n",
    "## Common NLP problems\n",
    "\n",
    "---\n",
    "\n",
    "The table below details some of the most common problems and tasks in the vast field of natural language processing (NLP).\n",
    "\n",
    "|Topic|Description|\n",
    "|-|-|\n",
    "| **Sentiment Analysis** | Is what is written positive or negative? | \n",
    "| **Named Entity Recognition** | Classify names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. |\n",
    "| **Summarization** | Boiling down large bodies of text to paraphrased versions |\n",
    "| **Topic Modeling** | What topics does a body of text belong to? (ie: Auto tagging of news articles) |\n",
    "| **Question answering** | Given a human-language question, determine its answer. |\n",
    "| **Word disambiguation** | Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or from an online resource such as WordNet. |\n",
    "| **Machine dialog systems** | Building response systems that react contextually to human input (ie: me: Siri, cook me some bacon.  Siri:  How do you like your bacon? ) | \n",
    "\n",
    "\n",
    "See Also:\n",
    "\n",
    "- [News Headline Anlaysis](http://nbviewer.jupyter.org/github/AYLIEN/headline_analysis/blob/06f1223012d285412a650c201a19a1c95859dca1/main-chunks.ipynb?utm_content=buffer5d40c&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)\n",
    "- [Sentiment + Robot Classification in Movies](http://nbviewer.jupyter.org/github/cojette/ClusteringRobotsinMovie/blob/master/Classification%20of%20Robots%20in%20Movies.ipynb)\n",
    "- [Text Summarization / Gensim](http://nbviewer.jupyter.org/github/piskvorky/gensim/blob/develop/docs/notebooks/summarization_tutorial.ipynb)\n",
    "- [Sentiment Analysis Intro](http://nbviewer.jupyter.org/github/sgsinclair/alta/blob/master/ipynb/SentimentAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='models'></a>\n",
    "## Common NLP models and terms\n",
    "\n",
    "---\n",
    "\n",
    "- LSI (Latent Semantic Indexing)\n",
    "- LDA (Latent Dirichlet Allocation)\n",
    "- HDP (Hierarchical Dirichlet Process)\n",
    "- Word2Vec\n",
    "- LogisticRegression\n",
    "- Naive Bayes\n",
    "- SVM\n",
    "- CountVectorizer\n",
    "- TfIdF (term frequency inverse document frequency)\n",
    "- DTM (document term matrix)\n",
    "\n",
    "> **Note:** This is not an exhaustive list, nor will we be covering all of these models in class. NLP is a very deep and very broad area of data science that could warrant it's own immersive entirely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='simple'></a>\n",
    "## A Simple Example\n",
    "---\n",
    "\n",
    "Suppose we are building a spam classifier. Inputs are emails and the output is a binary label.\n",
    "\n",
    "Here's an example of an input email from each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
      "\n",
      "\n",
      "\n",
      "Hello, \n",
      "I am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam = \"\"\"\n",
    "Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "\"\"\"\n",
    "\n",
    "ham = \"\"\"\n",
    "Hello, \\nI am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
    "\"\"\"\n",
    "print(spam)\n",
    "print()\n",
    "print(ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Can you think of a simple heuristic rule to catch email like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "By defining a simple rule that parses the text for presence of keywords we are performing one of the simplest text feature extraction methods: _binary word counting_.\n",
    "\n",
    "\n",
    "<a id='bow'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag-of-words / word counting\n",
    "---\n",
    "\n",
    "The bag-of-words model is a simplified representation of the raw data. In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words.\n",
    "\n",
    "Bag-of-words representations discard grammar, order, and structure in the text, but track occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 7, 'of': 4, 'and': 3, 'your': 2, 'contact': 2, 'have': 2, 'to': 2, 'an': 2, 'this': 2, 'is': 2, 'am': 2, 'in': 2, 'with': 2, 'the': 2, 'years': 2, 'etc.': 2, 'hello,': 1, 'saw': 1, 'information': 1, 'on': 1, 'linkedin.': 1, 'carefully': 1, 'read': 1, 'through': 1, 'profile': 1, 'you': 1, 'seem': 1, 'outstanding': 1, 'personality.': 1, 'one': 1, 'major': 1, 'reason': 1, 'why': 1, 'you.': 1, 'my': 1, 'name': 1, 'mr.': 1, 'valery': 1, 'grayfer': 1, 'chairman': 1, 'board': 1, 'directors': 1, 'pjsc': 1, '\"lukoil\".': 1, '86': 1, 'old': 1, 'was': 1, 'diagnosed': 1, 'cancer': 1, '2': 1, 'ago.': 1, 'will': 1, 'be': 1, 'going': 1, 'for': 1, 'operation': 1, 'later': 1, 'week.': 1, 'decided': 1, 'will/donate': 1, 'sum': 1, '8,750,000.00': 1, 'euros(eight': 1, 'million': 1, 'seven': 1, 'hundred': 1, 'fifty': 1, 'thousand': 1, 'euros': 1, 'only': 1})\n",
      "\n",
      "Counter({'to': 5, 'of': 4, 'you': 4, 'the': 3, 'i': 2, 'data': 2, 'scientist': 2, 'we': 2, 'and': 2, 'further': 2, 'hello,': 1, 'am': 1, 'writing': 1, 'in': 1, 'regards': 1, 'your': 1, 'application': 1, 'position': 1, 'at': 1, 'hooli': 1, 'x.': 1, 'are': 1, 'pleased': 1, 'inform': 1, 'that': 1, 'passed': 1, 'first': 1, 'round': 1, 'interviews': 1, 'would': 1, 'like': 1, 'invite': 1, 'for': 1, 'an': 1, 'on-site': 1, 'interview': 1, 'with': 1, 'our': 1, 'senior': 1, 'mr.': 1, 'john': 1, 'smith.': 1, 'will': 1, 'find': 1, 'attached': 1, 'this': 1, 'message': 1, 'information': 1, 'on': 1, 'date,': 1, 'time': 1, 'location': 1, 'interview.': 1, 'please': 1, 'let': 1, 'me': 1, 'know': 1, 'if': 1, 'can': 1, 'be': 1, 'any': 1, 'assistance.': 1, 'best': 1, 'regards.': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(spam.lower().split()))\n",
    "print()\n",
    "print(Counter(ham.lower().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> In the above example we counted the number of times each word appeared in the text. Note that since we included all the words in the text, we created a dictionary that contains many words with only one appearance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"countvectorizer\"></a>\n",
    "## Sklearn `CountVectorizer`\n",
    "---\n",
    "\n",
    "Sklearn offers a `CountVectorizer` class which basically does the same, but which has many configurable options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='\\\\w+', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvec = CountVectorizer(token_pattern='\\w+')\n",
    "cvec.fit([spam,ham])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The count vectorizer returns a sparse matrix (see [scipy](https://docs.scipy.org/doc/scipy/reference/sparse.html))\n",
    "\n",
    "In a sparse matrix, only those entries are stored which are different from zero. \n",
    "They are stored through triples of numbers, the occupied row and column index combination together with the value.\n",
    "\n",
    "This is particularly useful for NLP where each document will only contain a small amount of all the possible words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x115 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 134 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_matrix = cvec.transform([spam, ham])\n",
    "document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 2 2 3 0 0 0 0 0 0 1 0 1 0 1 1 1 2 0 0 1 1 1 1 1 2 2 1 0 0\n",
      "  1 0 1 1 2 1 0 1 7 0 2 0 1 0 0 0 2 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 4 1 1 1\n",
      "  1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 2 2 1 1 0 2 1 1 0 1 1 2\n",
      "  2 0 0 0 2 2 2]\n",
      " [0 0 0 0 0 0 0 1 1 2 1 1 1 1 1 1 1 1 0 1 0 0 0 0 2 1 0 0 0 0 0 0 0 0 1 1\n",
      "  1 2 0 0 0 1 1 0 2 1 1 1 1 2 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 4 0 2 0\n",
      "  0 0 1 0 1 0 0 1 1 1 0 0 0 2 1 0 2 0 1 0 1 1 0 1 3 1 0 0 1 5 0 0 2 0 0 1\n",
      "  1 1 1 1 0 4 1]]\n"
     ]
    }
   ],
   "source": [
    "print(document_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero entries:\n",
      "134\n",
      "Highest count:\n",
      "7\n",
      "Row means:\n",
      "[[0.86086957]\n",
      " [0.72173913]]\n",
      "Transform to numpy array format:\n",
      "[[1 1 1 1 1 1 1 2 2 3 0 0 0 0 0 0 1 0 1 0 1 1 1 2 0 0 1 1 1 1 1 2 2 1 0 0\n",
      "  1 0 1 1 2 1 0 1 7 0 2 0 1 0 0 0 2 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 4 1 1 1\n",
      "  1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 2 2 1 1 0 2 1 1 0 1 1 2\n",
      "  2 0 0 0 2 2 2]\n",
      " [0 0 0 0 0 0 0 1 1 2 1 1 1 1 1 1 1 1 0 1 0 0 0 0 2 1 0 0 0 0 0 0 0 0 1 1\n",
      "  1 2 0 0 0 1 1 0 2 1 1 1 1 2 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 4 0 2 0\n",
      "  0 0 1 0 1 0 0 1 1 1 0 0 0 2 1 0 2 0 1 0 1 1 0 1 3 1 0 0 1 5 0 0 2 0 0 1\n",
      "  1 1 1 1 0 4 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nonzero entries:\")\n",
    "print(document_matrix.nnz)\n",
    "print(\"Highest count:\")\n",
    "print(document_matrix.max())\n",
    "print(\"Row means:\")\n",
    "print(document_matrix.mean(axis=1))\n",
    "print(\"Transform to numpy array format:\")\n",
    "print(document_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting the result into a dataframe\n",
    "\n",
    "> **Note:** For huge text bodies continue working with the sparse matrix format. Many sklearn models can digest sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '2',\n",
       " '750',\n",
       " '8',\n",
       " '86',\n",
       " 'ago',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'application',\n",
       " 'are',\n",
       " 'assistance',\n",
       " 'at',\n",
       " 'attached',\n",
       " 'be',\n",
       " 'best',\n",
       " 'board',\n",
       " 'can',\n",
       " 'cancer',\n",
       " 'carefully',\n",
       " 'chairman',\n",
       " 'contact',\n",
       " 'data',\n",
       " 'date',\n",
       " 'decided',\n",
       " 'diagnosed',\n",
       " 'directors',\n",
       " 'donate',\n",
       " 'eight',\n",
       " 'etc',\n",
       " 'euros',\n",
       " 'fifty',\n",
       " 'find',\n",
       " 'first',\n",
       " 'for',\n",
       " 'further',\n",
       " 'going',\n",
       " 'grayfer',\n",
       " 'have',\n",
       " 'hello',\n",
       " 'hooli',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'inform',\n",
       " 'information',\n",
       " 'interview',\n",
       " 'interviews',\n",
       " 'invite',\n",
       " 'is',\n",
       " 'john',\n",
       " 'know',\n",
       " 'later',\n",
       " 'let',\n",
       " 'like',\n",
       " 'linkedin',\n",
       " 'location',\n",
       " 'lukoil',\n",
       " 'major',\n",
       " 'me',\n",
       " 'message',\n",
       " 'million',\n",
       " 'mr',\n",
       " 'my',\n",
       " 'name',\n",
       " 'of',\n",
       " 'old',\n",
       " 'on',\n",
       " 'one',\n",
       " 'only',\n",
       " 'operation',\n",
       " 'our',\n",
       " 'outstanding',\n",
       " 'passed',\n",
       " 'personality',\n",
       " 'pjsc',\n",
       " 'please',\n",
       " 'pleased',\n",
       " 'position',\n",
       " 'profile',\n",
       " 'read',\n",
       " 'reason',\n",
       " 'regards',\n",
       " 'round',\n",
       " 'saw',\n",
       " 'scientist',\n",
       " 'seem',\n",
       " 'senior',\n",
       " 'seven',\n",
       " 'site',\n",
       " 'smith',\n",
       " 'sum',\n",
       " 'that',\n",
       " 'the',\n",
       " 'this',\n",
       " 'thousand',\n",
       " 'through',\n",
       " 'time',\n",
       " 'to',\n",
       " 'valery',\n",
       " 'was',\n",
       " 'we',\n",
       " 'week',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'would',\n",
       " 'writing',\n",
       " 'x',\n",
       " 'years',\n",
       " 'you',\n",
       " 'your']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>of</th>\n",
       "      <th>and</th>\n",
       "      <th>your</th>\n",
       "      <th>is</th>\n",
       "      <th>contact</th>\n",
       "      <th>have</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>etc</th>\n",
       "      <th>...</th>\n",
       "      <th>find</th>\n",
       "      <th>our</th>\n",
       "      <th>passed</th>\n",
       "      <th>please</th>\n",
       "      <th>pleased</th>\n",
       "      <th>position</th>\n",
       "      <th>invite</th>\n",
       "      <th>data</th>\n",
       "      <th>regards</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   i  of  and  your  is  contact  have  the  this  etc  ...  find  our  \\\n",
       "0  7   4    3     2   2        2     2    2     2    2  ...     0    0   \n",
       "1  2   4    2     1   0        0     0    3     1    0  ...     1    1   \n",
       "\n",
       "   passed  please  pleased  position  invite  data  regards  like  \n",
       "0       0       0        0         0       0     0        0     0  \n",
       "1       1       1        1         1       1     2        2     1  \n",
       "\n",
       "[2 rows x 115 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(document_matrix.toarray(),\n",
    "                  columns=cvec.get_feature_names())\n",
    "\n",
    "df.transpose().sort_values(0, ascending=False).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i      9\n",
       "of     8\n",
       "to     7\n",
       "you    6\n",
       "the    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis=0).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    99\n",
       "1    83\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ### Spend a couple of minutes scanning the documentation to figure out what those parameters do. \n",
    "\n",
    "Share a few takeaways from the documentation. What arguments and capabilities stand out to you? How do the arguments affect the parsing behavior?\n",
    "\n",
    "[Count Vectorizer Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tfidf\"></a>\n",
    "## Term frequency - inverse document frequency (tf-idf)\n",
    "\n",
    "---\n",
    "\n",
    "A tf-idf score tells us which words are most discriminating between documents. Words that occur a lot in one document but don't occur in many documents contain a great deal of discriminating power.\n",
    "\n",
    "- This weight is a statistical measure used to evaluate how important a word is to a document in a collection (aka corpus).\n",
    "- The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word plus one, obtained by dividing the total number of documents by the number of documents containing the term plus one, and then taking the logarithm of that quotient.\n",
    "\n",
    "This enhances terms that are highly specific of a particular document, while suppressing terms that are common to most documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how it is calculated:\n",
    "\n",
    "Term frequency `tf` is the frequency of a certain term in a document:\n",
    "\n",
    "$$\n",
    "\\mathrm{tf}(t,d) = N_\\text{term}\n",
    "$$\n",
    "\n",
    "Inverse document frequency `idf` is defined as the frequency of documents that contain that term over the whole corpus (logarithmically scaled and adjusted to give only positive results greater or equal to one):\n",
    "\n",
    "$$\n",
    "\\mathrm{idf}(t, D) = 1+\\log\\left(\\frac{1+N_\\text{Documents}}{1+N_\\text{Documents that contain term}}\\right)\n",
    "$$\n",
    "\n",
    "Term frequency - Inverse Document Frequency (`tf-idf`) is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t,d,D) = \\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n",
    "\n",
    "Usually the obtained numbers are then rescaled in such a way that the tf-idf vector of each document has Euclidean length one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example\n",
    "\n",
    "Consider the three sentences:\n",
    "\n",
    "    a) I have a cat.\n",
    "\n",
    "    b) I have a puppy.\n",
    "    \n",
    "    c) I have a dog, I have a kitten, and I have a pen.\n",
    "    \n",
    "- Calculate the tf-idf of the words `cat`, `have` and `puppy`.\n",
    "- On paper, sketch the values obtained in a 3-dimensional coordinate system.\n",
    "- Follow the default sklearn settings which will discard single letter words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Tf-idf calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Word `have`:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "{\\rm tf}({\\rm have},a) &=& 1 \\\\\n",
    "{\\rm tf}({\\rm have},b) &=& 1 \\\\\n",
    "{\\rm tf}({\\rm have},c) &=& 3 \\\\\n",
    "{\\rm idf}({\\rm have},a,D) &=& \n",
    "1+\\log\\left(\\frac{1+3}{1+3}\\right) = 1 \\\\\n",
    "{\\rm idf}({\\rm have},b,D) &=& \n",
    "1+\\log\\left(\\frac{1+3}{1+3}\\right) = 1 \\\\\n",
    "{\\rm idf}({\\rm have},c,D) &=& \n",
    "1+\\log\\left(\\frac{1+3}{1+3}\\right) = 1 \\\\\n",
    "{\\rm tf-idf}({\\rm have},a,D) &=& 1\\\\\n",
    "{\\rm tf-idf}({\\rm have},b,D) &=& 1 \\\\\n",
    "{\\rm tf-idf}({\\rm have},c,D) &=& 3\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- Word `cat`:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "{\\rm tf}({\\rm cat},a) &=& 1 \\\\\n",
    "{\\rm idf}({\\rm cat},a,D) &=& \n",
    "1+\\log\\left(\\frac{1+3}{1+1}\\right) = 1+\\log(2) \\\\\n",
    "{\\rm tf-idf}({\\rm cat},a,D) &=& 1+\\log(2)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- Word `puppy`:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "{\\rm tf}({\\rm puppy},b) &=& 1 \\\\\n",
    "{\\rm idf}({\\rm puppy},b,D) &=& \n",
    "1+\\log\\left(\\frac{1+3}{1+1}\\right) = 1+\\log(2) \\\\\n",
    "{\\rm tf-idf}({\\rm puppy},b,D) &=& 1+\\log(2)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- Words `dog`, `kitten`, `pen`:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "{\\rm tf}({\\rm dog},c) &=& 1 \\\\\n",
    "{\\rm tf}({\\rm kitten},c) &=& 1 \\\\\n",
    "{\\rm tf}({\\rm pen},c) &=& 1 \\\\\n",
    "{\\rm idf}({\\rm dog},c,D) &=& \n",
    "1+\\log\\left(\\frac{1+3}{1+1}\\right) = 1+\\log(2) \\\\\n",
    "{\\rm idf}({\\rm kitten},c,D) &=& \n",
    "1+\\log\\left(\\frac{1+3}{1+1}\\right) = 1+\\log(2) \\\\\n",
    "{\\rm idf}({\\rm pen},c,D) &=& \n",
    "1+\\log\\left(\\frac{1+3}{1+1}\\right) = 1+\\log(2) \\\\\n",
    "{\\rm tf-idf}({\\rm dog},c,D) &=& 1+\\log(2)\\\\\n",
    "{\\rm tf-idf}({\\rm kitten},c,D) &=& 1+\\log(2)\\\\\n",
    "{\\rm tf-idf}({\\rm pen},c,D) &=& 1+\\log(2)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- Word `and`:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "{\\rm tf}({\\rm and},c) &=& 1 \\\\\n",
    "{\\rm idf}({\\rm and},c,D) &=& \n",
    "1+\\log\\left(\\frac{1+3}{1+1}\\right) = 1+\\log(2) \\\\\n",
    "{\\rm tf-idf}({\\rm and},c,D) &=& 1+\\log(2)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Tf-Idf matrix:\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\begin{array}{ccccccc}\n",
    "{\\rm and}&{\\rm cat}&{\\rm dog}&{\\rm have}&{\\rm kitten}\n",
    "&{\\rm pen}&{\\rm puppy}\\\\\n",
    "0&1.693&0&1&0&0&0\\\\\n",
    "0&0&0&1&0&0&1.693\\\\\n",
    "1.693&0&1.693&3&1.693&1.693&0\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Still have to normalize the tf-idf vectors along the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8610369959439764\n",
      "0.5085423203783267\n",
      "0.37425510099849485\n",
      "0.6631232747434112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize along rows\n",
    "a = 1+np.log(2)\n",
    "print(a/(a**2+1**2)**0.5)\n",
    "print(1/(a**2+1**2)**0.5)\n",
    "print(a/(4*a**2+3**2)**0.5)\n",
    "print(3/(4*a**2+3**2)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id='tfidf-vec'></a>\n",
    "## Sklearn `TfidfVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use TFIDF?\n",
    "\n",
    "- Common words are penalized\n",
    "- Rare words have more influence\n",
    "\n",
    "Sklearn provides a tf-idf vectorizer that works similarly to the other vectorizers we've covered. Notice that we can also eliminate stop words to improve our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the `TfidfVectorizer` to fit the spam and ham data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "tvec.fit([spam, ham])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "      <th>euros</th>\n",
       "      <th>contact</th>\n",
       "      <th>personality</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>lukoil</th>\n",
       "      <th>major</th>\n",
       "      <th>million</th>\n",
       "      <th>old</th>\n",
       "      <th>operation</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>message</th>\n",
       "      <th>location</th>\n",
       "      <th>let</th>\n",
       "      <th>know</th>\n",
       "      <th>john</th>\n",
       "      <th>invite</th>\n",
       "      <th>interviews</th>\n",
       "      <th>interview</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>0.155195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         years     euros   contact  personality  linkedin    lukoil     major  \\\n",
       "spam  0.290133  0.290133  0.290133     0.145067  0.145067  0.145067  0.145067   \n",
       "ham   0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       million       old  operation  ...      date   message  location  \\\n",
       "spam  0.145067  0.145067   0.145067  ...  0.000000  0.000000  0.000000   \n",
       "ham   0.000000  0.000000   0.000000  ...  0.155195  0.155195  0.155195   \n",
       "\n",
       "           let      know      john    invite  interviews  interview      like  \n",
       "spam  0.000000  0.000000  0.000000  0.000000    0.000000    0.00000  0.000000  \n",
       "ham   0.155195  0.155195  0.155195  0.155195    0.155195    0.31039  0.155195  \n",
       "\n",
       "[2 rows x 68 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tvec.transform([spam, ham]).toarray(),\n",
    "                  columns=tvec.get_feature_names(),\n",
    "                  index=['spam', 'ham'])\n",
    "\n",
    "df.transpose().sort_values('spam', ascending=False).head(100).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the stopwords used by sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = tvec.get_stop_words()\n",
    "sorted(list(stopwords))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the above example\n",
    "\n",
    "```python\n",
    "test = ['I have a cat', 'I have a puppy', 'I have a dog, I have a kitten, and I have a pen']\n",
    "```\n",
    "\n",
    "Compare using or not using stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>have</th>\n",
       "      <th>kitten</th>\n",
       "      <th>pen</th>\n",
       "      <th>puppy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.693</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.693</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.693</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.693</td>\n",
       "      <td>1.693</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     and    cat    dog  have  kitten    pen  puppy\n",
       "0  0.000  1.693  0.000   1.0   0.000  0.000  0.000\n",
       "1  0.000  0.000  0.000   1.0   0.000  0.000  1.693\n",
       "2  1.693  0.000  1.693   3.0   1.693  1.693  0.000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['I have a cat', 'I have a puppy',\n",
    "        'I have a dog, I have a kitten, and I have a pen']\n",
    "\n",
    "tvec = TfidfVectorizer(norm=None)\n",
    "tvec.fit(test)\n",
    "\n",
    "tvec_mat = pd.DataFrame(np.round(tvec.fit_transform(test).toarray(), 3),\n",
    "                        columns=tvec.get_feature_names())\n",
    "tvec_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>have</th>\n",
       "      <th>kitten</th>\n",
       "      <th>pen</th>\n",
       "      <th>puppy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.374</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     and    cat    dog   have  kitten    pen  puppy\n",
       "0  0.000  0.861  0.000  0.509   0.000  0.000  0.000\n",
       "1  0.000  0.000  0.000  0.509   0.000  0.000  0.861\n",
       "2  0.374  0.000  0.374  0.663   0.374  0.374  0.000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['I have a cat', 'I have a puppy',\n",
    "        'I have a dog, I have a kitten, and I have a pen']\n",
    "\n",
    "tvec = TfidfVectorizer(norm='l2')\n",
    "tvec.fit(test)\n",
    "\n",
    "tvec_mat = pd.DataFrame(np.round(tvec.fit_transform(test).toarray(), 3),columns=tvec.get_feature_names())\n",
    "tvec_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.000402\n",
       "1    1.000402\n",
       "2    0.999073\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Euclidean norm of row vectors is one up to rounding\n",
    "tvec_mat.applymap(lambda x: x**2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"downsides-bow\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Downsides to bag-of-words\n",
    "\n",
    "---\n",
    "\n",
    "Bag-of-word approaches like the one outlined above completely ignore the structure of a sentence. Bag-of-word approaches merely assess presence of specific words or word combinations.\n",
    "\n",
    "The same word can have multiple meanings in different contexts. Consider for example the following two sentences:\n",
    "\n",
    "- There's wood floating in the **sea**.\n",
    "- Mike's in a **sea** of trouble with the move.\n",
    "\n",
    "How do we teach a computer to disambiguate? Later we will cover some other techniques that may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='segmentation'></a>\n",
    "## Segmentation\n",
    "\n",
    "---\n",
    "\n",
    "_Segmentation_ is a technique to **identify sentences** within a body of text. Language is not a continuous uninterrupted stream of words: punctuation serves as a guide to group together words that convey meaning when contiguous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " ' What do you think of that?',\n",
       " ' I bet you hate it!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "\n",
    "easy_split_text = [\"I went to the zoo today.\",\n",
    "                   \"What do you think of that?\",\n",
    "                   \"I bet you hate it!\",\n",
    "                   \"Or maybe you don't\"]\n",
    "\n",
    "\n",
    "def simple_sentencer(text):\n",
    "    '''take a string called `text` and return\n",
    "    a list of strings, each containing a sentence'''\n",
    "\n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "    return sentences\n",
    "\n",
    "\n",
    "simple_sentencer(easy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`# Result:`\n",
    "\n",
    "    ['I went to the zoo today.',\n",
    "     ' What do you think of that?',\n",
    "     ' I bet you hate it!']\n",
    "\n",
    "The function above doesn't work perfectly. On the other hand, the python NLTK library offers a more robust and easy to use sentencer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nltk-sentencer'></a>\n",
    "- ### There's an easier way to do the same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " 'What do you think of that?',\n",
       " 'I bet you hate it!',\n",
       " \"Or maybe you don't\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "sent_detector = PunktSentenceTokenizer()\n",
    "sent_detector.sentences_from_text(easy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"install\"></a>\n",
    "### Install NLTK packages\n",
    "\n",
    "First, in your terminal, run \n",
    "\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "Then within python, run the following:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ### Use `nltk.download()` to explore the available packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"stem-nltk\"></a>\n",
    "## Stemming with NLTK\n",
    "\n",
    "---\n",
    "\n",
    "**Text normalization** is the process of converting slightly different versions of words with essentially equivalent meaning into the same features.\n",
    "\n",
    "For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\".\n",
    "\n",
    "- ### What are other common cases of text that could need normalization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    - Person titles (Mr, MR, Dr etc.)\n",
    "    - Dates (10/03, March 10 etc.)\n",
    "    - Numbers\n",
    "    - Plurals\n",
    "    - Verb conjugations\n",
    "    - Slang\n",
    "    - SMS abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It would be wrong to consider the words \"MR.\" and \"mr\" to be different features, thus we need a technique to normalize words to a common root. This technique is called _stemming_.\n",
    "\n",
    "- Science, Scientist => Scien\n",
    "- Swimming, Swimmer, Swim => Swim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did above we could define a Stemmer based on rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Science', 'Scientist', 'Scientists']\n",
      "['Scien', 'Scien', 'Scien']\n",
      "['player', 'plays', 'playing']\n",
      "['play', 'play', 'play']\n"
     ]
    }
   ],
   "source": [
    "def stem(tokens):\n",
    "    '''rule-based stemming of a bunch of tokens'''\n",
    "\n",
    "    new_bag = []\n",
    "    for token in tokens:\n",
    "        # define rules here\n",
    "        if token.endswith('s'):\n",
    "            new_bag.append(token[:-1])\n",
    "        elif token.endswith('er'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('ce'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('tion'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('tist'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('ing'):\n",
    "            new_bag.append(token[:-3])\n",
    "        else:\n",
    "            new_bag.append(token)\n",
    "\n",
    "    return new_bag\n",
    "\n",
    "\n",
    "list_science = ['Science', 'Scientist', 'Scientists']\n",
    "print(list_science)\n",
    "list_stem = stem(stem(list_science))\n",
    "print(list_stem)\n",
    "list_play = ['player', 'plays', 'playing']\n",
    "print(list_play)\n",
    "print(stem(list_play))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily for us, NLTK contains several robust stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('walks'))\n",
    "print(stemmer.stem('walked'))\n",
    "print(stemmer.stem('Walking'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='group'></a>\n",
    "### Stemming approaches\n",
    "\n",
    "\n",
    "> There are other stemmers available in NLTK. Look at [this article](https://www.elastic.co/guide/en/elasticsearch/guide/current/choosing-a-stemmer.html) to find out about different stemmers. Have a look how it works in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>\n",
    "## Stop words\n",
    "\n",
    "---\n",
    "\n",
    "Some words are very common and provide no legitimate information about the content of the text.\n",
    "\n",
    "- ### Can you give some examples?\n",
    "\n",
    "> We should remove these _stop words_. Note that each language has different stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/paxton615/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop = stopwords.words('english') 这个english的stopwords在Users/paxton615/nltk_data/corpora/stopwords/english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo', 'bar', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "sentence = \"this is a foo bar sentence\"\n",
    "print([i for i in sentence.split() if i not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(stop)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pos'></a>\n",
    "## Part of speech tagging\n",
    "\n",
    "---\n",
    "\n",
    "Each word has a specific role in a sentence (Verb, Noun, etc.). Parts-of-speech tagging (POS) is a feature extraction technique that attaches a tag to each word in the sentence in order to provide a more precise context for further analysis. This is often a resource intensive process, but it can sometimes improve the accuracy of our models to have the grammatical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'today': 'NN',\n",
       " 'is': 'VBZ',\n",
       " 'a': 'DT',\n",
       " 'great': 'JJ',\n",
       " 'day': 'NN',\n",
       " 'to': 'TO',\n",
       " 'learn': 'VB',\n",
       " 'nlp': 'NN'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "tags = dict(pos_tag(tok.tokenize(\"today is a great day to learn nlp\")))\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the explanation for the abbreviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nltk.help.upenn_tagset(tagpattern=None)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.help.upenn_tagset()\n",
    "nltk.help.upenn_tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mtagsets\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets')\n  \u001b[0m\n  Attempted to load \u001b[93mhelp/tagsets/PY3/upenn_tagset.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/paxton615/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/share/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ee5f6fdf16bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# only for the ones met in our example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupenn_tagset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/help.py\u001b[0m in \u001b[0;36mupenn_tagset\u001b[0;34m(tagpattern)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupenn_tagset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_format_tagset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"upenn_tagset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/help.py\u001b[0m in \u001b[0;36m_format_tagset\u001b[0;34m(tagset, tagpattern)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_format_tagset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtagdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"help/tagsets/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtagset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0m_print_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtagsets\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets')\n  \u001b[0m\n  Attempted to load \u001b[93mhelp/tagsets/PY3/upenn_tagset.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/paxton615/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/share/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# only for the ones met in our example\n",
    "for val in list(set(tags.values())):\n",
    "    print(nltk.help.upenn_tagset(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='unicode'></a>\n",
    "## Unicode: a common pitfall\n",
    "\n",
    "What happens when we get a character that is referenced outside of the character space, for instance a German umlaut **&ouml;** or a Japanese Katakana character  **片仮名 / カタカナ**?\n",
    "\n",
    "\n",
    "- Python doesn't know how to handle these characters if it has to process it in any way\n",
    "- Characters outside the Latin character space will get converted to ordinal 0\n",
    "- This problem can be very frustrating to deal with\n",
    "\n",
    "Luckily, sklearn has robust classes for text feature extraction. Use sklearn's built-in text preprocessing method when possible.  Always save/encode your text as UTF8 when there are options available to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "In this lesson we obtained an overview of Natural Language Processing (NLP) and learned about two very powerful toolkits:\n",
    "- Scikit Learn Feature Extraction Text\n",
    "- Natural Language Tool Kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Some real world applications of these techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spam Detection\n",
    "- Preprocessing for larger NLP problems\n",
    "- Job market analysis\n",
    "- Crude topic analyis\n",
    "- Building a keyword extractation heuristic and piping it into a marketing analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- Check out this [Yelp blog post](http://engineeringblog.yelp.com/2015/09/automatically-categorizing-yelp-businesses.html) how they completed a classification task (with over 1000 response variables!) using restaurant review text\n",
    "- A list of all stop-words is available [here](https://github.com/ga-students/DSI-DC-2/blob/master/curriculum/Week-05/5.04-nlp/stop-words.txt) h/t sleevillanueva\n",
    "- This lesson made use of Charlie Greenbacher's [Intro to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf), which he delivered at the [DC-NLP Meetup](http://www.meetup.com/DC-NLP/) \n",
    "- Wikipedia includes a [walkthrough](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) of TF-IDF\n",
    "- Play with Google's [ngram tool](https://books.google.com/ngrams/graph?content=data+science&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1%3B%2Cdata%20science%3B%2Cc0)\n",
    "- A hilarious data scientist gone rogue used NLP and Eigenfaces (Eigenvalues for face recognition) [for Tinder](http://dataconomy.com/hacking-tinder-with-facial-recognition-nlp/)\n",
    "- Check out KPCB's 2016 internet trends [this massive, insightful deck](http://www.kpcb.com/internet-trends)\n",
    "- [Choosing a Stemmer](https://www.elastic.co/guide/en/elasticsearch/guide/current/choosing-a-stemmer.html)\n",
    "- Check documentation: [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html),  [TF-IDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- [Term Frequency Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "270px",
    "left": "0px",
    "right": "570.286px",
    "top": "145px",
    "width": "153px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
