{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Pipelines in Sklearn\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "### Core\n",
    "- Learn what an sklearn pipeline is and scenarios where they are useful\n",
    "- Standardize data as part of a pipeline\n",
    "- Use pipelines with training and testing data\n",
    "- Use the `make_pipeline` function to easily create pipeline objects\n",
    "\n",
    "### Target\n",
    "- Be able to build a custom transformation in sklearn and use it in a pipeline\n",
    "- Investigate the internals of sklearn pipelines\n",
    "\n",
    "### Stretch\n",
    "- Understand the concepts behind transformers and estimators\n",
    "- Get an intuition of how we can leverage the pipeline with gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1\">Learning Objectives</a></span><ul class=\"toc-item\"><li><span><a href=\"#Core\" data-toc-modified-id=\"Core-1.1\">Core</a></span></li><li><span><a href=\"#Target\" data-toc-modified-id=\"Target-1.2\">Target</a></span></li><li><span><a href=\"#Stretch\" data-toc-modified-id=\"Stretch-1.3\">Stretch</a></span></li></ul></li><li><span><a href=\"#Introduction-to-pipelines\" data-toc-modified-id=\"Introduction-to-pipelines-2\">Introduction to pipelines</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-titanic-data\" data-toc-modified-id=\"Load-the-titanic-data-2.1\">Load the titanic data</a></span></li></ul></li><li><span><a href=\"#Loading-the-pipeline-objects\" data-toc-modified-id=\"Loading-the-pipeline-objects-3\">Loading the pipeline objects</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-titanic-data\" data-toc-modified-id=\"The-titanic-data-3.1\">The titanic data</a></span></li></ul></li><li><span><a href=\"#Preprocessing-steps-for-the-titanic-data\" data-toc-modified-id=\"Preprocessing-steps-for-the-titanic-data-4\">Preprocessing steps for the titanic data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-a-few-categorical-variables\" data-toc-modified-id=\"Check-for-a-few-categorical-variables-4.1\">Check for a few categorical variables</a></span></li></ul></li><li><span><a href=\"#Select-feature-and-target-variables\" data-toc-modified-id=\"Select-feature-and-target-variables-5\">Select feature and target variables</a></span></li><li><span><a href=\"#Standardize-the-data-and-fit-a-LogisticRegression-model\" data-toc-modified-id=\"Standardize-the-data-and-fit-a-LogisticRegression-model-6\">Standardize the data and fit a LogisticRegression model</a></span></li><li><span><a href=\"#Add-a-train-test-split\" data-toc-modified-id=\"Add-a-train-test-split-7\">Add a train-test split</a></span></li><li><span><a href=\"#Use-a-pipeline-to-standardize-the-data-and-fit-the-model\" data-toc-modified-id=\"Use-a-pipeline-to-standardize-the-data-and-fit-the-model-8\">Use a pipeline to standardize the data and fit the model</a></span></li><li><span><a href=\"#Using-pipelines-with-training-and-testing-data\" data-toc-modified-id=\"Using-pipelines-with-training-and-testing-data-9\">Using pipelines with training and testing data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Split-up-into-training-and-testing-X,-y,-fit-on-the-training-data-and-score-on-training-and-test-data\" data-toc-modified-id=\"Split-up-into-training-and-testing-X,-y,-fit-on-the-training-data-and-score-on-training-and-test-data-9.0.1\">Split up into training and testing X, y, fit on the training data and score on training and test data</a></span></li></ul></li><li><span><a href=\"#Exercise:-Experiment-by-setting-up-a-pipeline-using-different-scaling-methods-or-models\" data-toc-modified-id=\"Exercise:-Experiment-by-setting-up-a-pipeline-using-different-scaling-methods-or-models-9.1\">Exercise: Experiment by setting up a pipeline using different scaling methods or models</a></span></li></ul></li><li><span><a href=\"#Built-in-transformations-and-preprocessing-steps\" data-toc-modified-id=\"Built-in-transformations-and-preprocessing-steps-10\">Built-in transformations and preprocessing steps</a></span></li><li><span><a href=\"#Custom-transformations\" data-toc-modified-id=\"Custom-transformations-11\">Custom transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Custom-transformer-classes-start-with-this-template-code:\" data-toc-modified-id=\"Custom-transformer-classes-start-with-this-template-code:-11.1\">Custom transformer classes start with this template code:</a></span></li><li><span><a href=\"#Add-functions-to-the-class\" data-toc-modified-id=\"Add-functions-to-the-class-11.2\">Add functions to the class</a></span></li><li><span><a href=\"#Test-the-preprocessing-function\" data-toc-modified-id=\"Test-the-preprocessing-function-11.3\">Test the preprocessing function</a></span></li><li><span><a href=\"#Use-the-custom-TitanticPreprocessor-in-a-pipeline\" data-toc-modified-id=\"Use-the-custom-TitanticPreprocessor-in-a-pipeline-11.4\">Use the custom <code>TitanticPreprocessor</code> in a pipeline</a></span></li></ul></li><li><span><a href=\"#Looking-at-pipeline-internals-with-.get_params()\" data-toc-modified-id=\"Looking-at-pipeline-internals-with-.get_params()-12\">Looking at pipeline internals with <code>.get_params()</code></a></span></li><li><span><a href=\"#The-make_pipeline()-convenience-function\" data-toc-modified-id=\"The-make_pipeline()-convenience-function-13\">The <code>make_pipeline()</code> convenience function</a></span></li><li><span><a href=\"#Feature-Union\" data-toc-modified-id=\"Feature-Union-14\">Feature Union</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Extractor\" data-toc-modified-id=\"Feature-Extractor-14.1\">Feature Extractor</a></span></li><li><span><a href=\"#Create-a-dummifyer-class\" data-toc-modified-id=\"Create-a-dummifyer-class-14.2\">Create a dummifyer class</a></span></li><li><span><a href=\"#Build-the-feature-union\" data-toc-modified-id=\"Build-the-feature-union-14.3\">Build the feature union</a></span></li></ul></li><li><span><a href=\"#Independent-Practice\" data-toc-modified-id=\"Independent-Practice-15\">Independent Practice</a></span><ul class=\"toc-item\"><li><span><a href=\"#Improve-the-model-using-grid-search\" data-toc-modified-id=\"Improve-the-model-using-grid-search-15.1\">Improve the model using grid search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gridsearch-with-pipeline\" data-toc-modified-id=\"Gridsearch-with-pipeline-15.1.1\">Gridsearch with pipeline</a></span></li><li><span><a href=\"#Use-different-scaling\" data-toc-modified-id=\"Use-different-scaling-15.1.2\">Use different scaling</a></span></li><li><span><a href=\"#Binarize-predictor-variables\" data-toc-modified-id=\"Binarize-predictor-variables-15.1.3\">Binarize predictor variables</a></span></li><li><span><a href=\"#Use-polynomial-features\" data-toc-modified-id=\"Use-polynomial-features-15.1.4\">Use polynomial features</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-16\">Conclusions</a></span></li><li><span><a href=\"#Additional-resources\" data-toc-modified-id=\"Additional-resources-17\">Additional resources</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to pipelines\n",
    "\n",
    "---\n",
    "\n",
    "Often when working with data the same \"process\" is repeated multiple times, which can become tedious to recode. A simple example of this is doing the standardization of data before using regularized regression or other models.\n",
    "\n",
    "Luckily, sklearn has \"Pipelines\" that chain together multiple steps in a data analysis process. By constructing these you can consolidate all of the steps you went through into a single object.\n",
    "\n",
    "This codealong introduces how to use these pipelines and also serves as object oriented programming practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\n",
    "    '../../../../resource-datasets/titanic/titanic_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pipeline objects\n",
    "\n",
    "---\n",
    "\n",
    "From the `sklearn.pipeline` module we are going to import `Pipeline` and `make_pipeline`.\n",
    "\n",
    "`Pipeline` is the class object that will hold our data analysis process. The `make_pipeline` function is a convenience method that takes in a series of estimators or preprocessing steps and returns a `Pipeline` object.\n",
    "\n",
    "We'll start with the more explicit construction using `Pipeline` and then move on to the convenience function.\n",
    "\n",
    "[sklearn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"pipeline\" is jargon for a series of concatenated data transformations. Each stage of a pipeline feeds from the previous stage, i.e. the output of a stage is plugged into the input of the next stage and data flows through the pipeline from beginning to end.\n",
    "\n",
    "\n",
    "![pipeline](./assets/pipeline.png)\n",
    "\n",
    "---\n",
    "\n",
    "Pipelines provide a higher level of abstraction than the individual building blocks of a data science process and are a nice and convenient way to organize analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The titanic data\n",
    "\n",
    "What are preprocessing steps that you would carry out before fitting a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch     Fare Embarked  \n",
       "0      0   7.2500        S  \n",
       "1      0  71.2833        C  \n",
       "2      0   7.9250        S  \n",
       "3      0  53.1000        S  \n",
       "4      0   8.0500        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>448.589888</td>\n",
       "      <td>0.404494</td>\n",
       "      <td>2.240169</td>\n",
       "      <td>29.642093</td>\n",
       "      <td>0.514045</td>\n",
       "      <td>0.432584</td>\n",
       "      <td>34.567251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>258.683191</td>\n",
       "      <td>0.491139</td>\n",
       "      <td>0.836854</td>\n",
       "      <td>14.492933</td>\n",
       "      <td>0.930692</td>\n",
       "      <td>0.854181</td>\n",
       "      <td>52.938648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>222.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>445.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.645850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>677.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   712.000000  712.000000  712.000000  712.000000  712.000000   \n",
       "mean    448.589888    0.404494    2.240169   29.642093    0.514045   \n",
       "std     258.683191    0.491139    0.836854   14.492933    0.930692   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     222.750000    0.000000    1.000000   20.000000    0.000000   \n",
       "50%     445.000000    0.000000    2.000000   28.000000    0.000000   \n",
       "75%     677.250000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    5.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  712.000000  712.000000  \n",
       "mean     0.432584   34.567251  \n",
       "std      0.854181   52.938648  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    8.050000  \n",
       "50%      0.000000   15.645850  \n",
       "75%      1.000000   33.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps for the titanic data\n",
    "\n",
    "---\n",
    "\n",
    "There are some preprocessing steps we're going to do before classifying whether or not passengers survived:\n",
    "\n",
    "- Remove unwanted columns\n",
    "- Convert categorical string or numeric columns to dummy coded columns\n",
    "- Standardize the predictor matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we'll do this manually and then later integrate it into the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for a few categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "['female' 'male']\n",
      "['C' 'Q' 'S']\n",
      "[0 1 2 3 4 5]\n",
      "[0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "for column in ['Pclass', 'Sex', 'Embarked', 'SibSp', 'Parch']:\n",
    "    print(np.sort(titanic[column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(df, columns):\n",
    "    return df.drop(columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummy_cols(df, columns):\n",
    "    for column in columns:\n",
    "        categories = np.sort(df[column].unique())\n",
    "        for category in categories[1:]:\n",
    "            df[column+'_'+str(category)\n",
    "               ] = df[column].map(lambda x: 1 if x == category else 0)\n",
    "        df = df.drop(column, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   Age  SibSp  Parch     Fare  Pclass_2  Pclass_3  Sex_male  \\\n",
       "0         0  22.0      1      0   7.2500         0         1         1   \n",
       "1         1  38.0      1      0  71.2833         0         0         0   \n",
       "2         1  26.0      0      0   7.9250         0         1         0   \n",
       "3         1  35.0      1      0  53.1000         0         0         0   \n",
       "4         0  35.0      0      0   8.0500         0         1         1   \n",
       "\n",
       "   Embarked_Q  Embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unwanted columns\n",
    "data = drop_cols(titanic, ['PassengerId', 'Name'])\n",
    "# dummify columns\n",
    "data = make_dummy_cols(data, ['Pclass', 'Sex', 'Embarked'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select feature and target variables\n",
    "\n",
    "\n",
    "Now we'll split the data up into the X, y predictor target format, standardize the X matrix, and fit a Logistic Regression model on Survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "y = X.pop('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 9), (712,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data and fit a LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8019662921348315"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_s = scaler.fit_transform(X)\n",
    "model.fit(X_s, y)\n",
    "model.score(X_s, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7931427142714271\n",
      "0.8072289156626506\n",
      "0.8130841121495327\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "print(cross_val_score(model, X_train, y_train, cv=5).mean())\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a pipeline to standardize the data and fit the model\n",
    "\n",
    "Next we're going to build a pipeline that can combine the steps. We combine the standard scaler and the logistic regression into a single \n",
    "pipeline object.\n",
    "\n",
    "In the pipeline we indicate the object used in each step and choose a name for each step. Except the last step, all objects included in the pipeline need to be equipped with a `fit` and a `transform` function. The last step only needs a `fit` function. To fit a model the whole pipeline is called with `fit` on the data.\n",
    "\n",
    "[Sklearn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler),\n",
    "                       ('model', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipelines combine both pre-processing and model building steps into a single object**. \n",
    "\n",
    "Rather than manually building transformations and then feeding them into the models, pipelines tie both of these steps together.\n",
    "\n",
    "Furthermore, pipelines are equipped with the methods of the final estimator step:\n",
    "\n",
    "- `fit()` methods\n",
    "- `predict()` and/or `predict_proba()`\n",
    "- `score()`\n",
    "- ... etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pipeline to fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8019662921348315"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipe.predict(X)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the different steps involved by calling `.steps` (returning a list) or `.namedstep (returning a dictionary). We can get values out of each of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " ('model',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "            tol=0.0001, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0, warm_start=False)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.6420927 ,  0.51404494,  0.43258427, 34.5672514 ,  0.24297753,\n",
       "        0.49859551,  0.63623596,  0.03932584,  0.77808989])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the column means infered with the standard scaler\n",
    "pipe.steps[0][1].mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60222826, -0.32648765, -0.05202477,  0.0934684 , -0.47528871,\n",
       "        -1.14705344, -1.24761907, -0.15821131, -0.1700639 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the coefficients determined with logistic regression\n",
    "pipe.named_steps['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('scaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('model',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "             tol=0.0001, verbose=0, warm_start=False))],\n",
       " 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " 'scaler__copy': True,\n",
       " 'scaler__with_mean': True,\n",
       " 'scaler__with_std': True,\n",
       " 'model__C': 1.0,\n",
       " 'model__class_weight': None,\n",
       " 'model__dual': False,\n",
       " 'model__fit_intercept': True,\n",
       " 'model__intercept_scaling': 1,\n",
       " 'model__max_iter': 100,\n",
       " 'model__multi_class': 'warn',\n",
       " 'model__n_jobs': None,\n",
       " 'model__penalty': 'l2',\n",
       " 'model__random_state': 1,\n",
       " 'model__solver': 'lbfgs',\n",
       " 'model__tol': 0.0001,\n",
       " 'model__verbose': 0,\n",
       " 'model__warm_start': False}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()['model__C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipelines with training and testing data\n",
    "\n",
    "---\n",
    "\n",
    "Next we'll split up this data into training and testing sets. One of the greatest benefits  to using pipelines is that the preprocessing steps before the model fitting retain the \"fit\" information from the training data to be applied to the testing data.\n",
    "\n",
    "In the pipeline we built above, for example, the first standardization step is \"fit\" on the data we put into it. This means that the `StandardScaler` object takes the mean and standard deviation of that data and performs the procedure with those values.\n",
    "\n",
    "It _also_ means that were we to predict or score on future data, the standard scaler in the pipeline would use the training data's mean and standard deviation to standardize that test data. This is what we want! You definitely don't want to standardize the training and testing data to their own means and standard deviations.\n",
    "\n",
    "There are many scenarios in which the test data is actually data that we have not collected yet. In this case, you need to save the standardization procedure you used on the training data to use on this future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split up into training and testing X, y, fit on the training data and score on training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7931427142714271\n",
      "0.8072289156626506\n",
      "0.8130841121495327\n"
     ]
    }
   ],
   "source": [
    "X = data.copy()\n",
    "y = X.pop('Survived')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "print(cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
    "print(pipe.score(X_train, y_train))\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Experiment by setting up a pipeline using different scaling methods or models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline(steps=[('MinMaxScaler', MinMaxScaler()),\n",
    "                       ('LRmodel', LogisticRegressionCV())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('MinMaxScaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('LRmodel', LogisticRegressionCV(Cs=10, class_weight=None, cv='warn', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='warn', n_jobs=None, penalty='l2',\n",
       "           random_state=None, refit=True, scoring=None, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MinMaxScaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       " ('LRmodel',\n",
       "  LogisticRegressionCV(Cs=10, class_weight=None, cv='warn', dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "             multi_class='warn', n_jobs=None, penalty='l2',\n",
       "             random_state=None, refit=True, scoring=None, solver='lbfgs',\n",
       "             tol=0.0001, verbose=0))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2702172 , -0.10158271,  0.04093027,  0.18249737, -0.0921731 ,\n",
       "        -0.86094336, -1.41908435, -0.13162534, -0.32719718]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.steps[1][1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('scaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('model',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "             tol=0.0001, verbose=0, warm_start=False))],\n",
       " 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " 'scaler__copy': True,\n",
       " 'scaler__with_mean': True,\n",
       " 'scaler__with_std': True,\n",
       " 'model__C': 1.0,\n",
       " 'model__class_weight': None,\n",
       " 'model__dual': False,\n",
       " 'model__fit_intercept': True,\n",
       " 'model__intercept_scaling': 1,\n",
       " 'model__max_iter': 100,\n",
       " 'model__multi_class': 'warn',\n",
       " 'model__n_jobs': None,\n",
       " 'model__penalty': 'l2',\n",
       " 'model__random_state': 1,\n",
       " 'model__solver': 'lbfgs',\n",
       " 'model__tol': 0.0001,\n",
       " 'model__verbose': 0,\n",
       " 'model__warm_start': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pipe.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in transformations and preprocessing steps\n",
    "\n",
    "---\n",
    "\n",
    "Sklearn comes with a wide variety of useful classes for preprocessing your data prior to model fitting that can be put into pipelines.\n",
    "\n",
    "These can be found in the `sklearn.preprocessing` module and you should feel free to familiarize yourself with them if you want to make use of them in your code:\n",
    "\n",
    "The preprocessing module comes loaded with many very useful pre-processing classes.\n",
    "\n",
    "**Data Manipulators**\n",
    "\n",
    "- Binarizer\n",
    "- KernelCenterer\n",
    "- MaxAbsScaler\n",
    "- MinMaxScaler\n",
    "- Normalizer\n",
    "- OneHotEncoder\n",
    "- PolynomialFeatures\n",
    "- RobustScaler\n",
    "- StandardScaler\n",
    "\n",
    "**Data Imputation**\n",
    "\n",
    "- Imputer\n",
    "\n",
    "**Function Transformer**\n",
    "\n",
    "- FunctionTransformer\n",
    "\n",
    "**Label Manipulators**\n",
    "\n",
    "- LabelBinarizer\n",
    "- LabelEncoder\n",
    "- MultiLabelBinarizer\n",
    "\n",
    "[Sklearn preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformations\n",
    "\n",
    "---\n",
    "\n",
    "It's not always possible to use a built-in transformation class to do what you want. In fact, it's likely that you're going to run into a scenario where you need a customized preprocessing step before model fitting.\n",
    "\n",
    "Let's take our titanic data, for example. Say we wanted a preprocessor that would remove the columns we didn't want and create the dummy-coded columns before sending it through to the standardization step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom transformer classes start with this template code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to import the template classes to create a class that works like an sklearn class\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# our \"TitanicPreprocessor\" is going to do the processing\n",
    "\n",
    "\n",
    "class TitanticPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, *args):\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on this class:\n",
    "\n",
    "1. We have to load in the `BaseEstimator` and `TransformerMixin` classes for our preprocessor to \"inherit\" from the class definition.\n",
    "- Your class must contain the functions `fit` and `transform`, which will be used to chain the processes together in our pipeline.\n",
    "- The `*args` argument tells the function to expect an arbitrary number of arguments after whatever arguments were listed explicitly.\n",
    "\n",
    "If you are confused about those classes, [this article]( http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/) gives a nice overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add functions to the class\n",
    "\n",
    "- Include the dummy-coding function we wrote above\n",
    "- Add a function which removes unnecessary columns\n",
    "- Modify the `transform` function to perform these preprocessing steps, returning the new DataFrame\n",
    "- The fit function does not need to be modified\n",
    "- Add a class attribute which contains the final column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanticPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns_to_drop=None, columns_to_dummify=None):\n",
    "        self.feature_names = []\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "        self.columns_to_dummify = columns_to_dummify\n",
    "\n",
    "    def _drop_unused_cols(self, X):\n",
    "        for col in self.columns_to_drop:\n",
    "            try:\n",
    "                X = X.drop(col, axis=1)\n",
    "            except:\n",
    "                pass\n",
    "        return X\n",
    "\n",
    "    def _make_dummy_cols(self, X):\n",
    "        for column in self.columns_to_dummify:\n",
    "            try:\n",
    "                categories = np.sort(X[column].unique())\n",
    "                for category in categories[1:]:\n",
    "                    X[column+'_'+str(category)] = X[column].map(\n",
    "                        lambda x: 1 if x == category else 0)\n",
    "                X = X.drop(column, axis=1)\n",
    "            except:\n",
    "                pass\n",
    "        return X\n",
    "\n",
    "    def transform(self, X, *args):\n",
    "        X = self._make_dummy_cols(X)\n",
    "        X = self._drop_unused_cols(X)\n",
    "        self.feature_names = X.columns\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TitanticPreprocessor(columns_to_drop=['PassengerId', 'Name'],\n",
       "           columns_to_dummify=['Sex', 'Pclass', 'Embarked'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tprep = TitanticPreprocessor(columns_to_drop=['PassengerId', 'Name'],\n",
    "                             columns_to_dummify=['Sex', 'Pclass', 'Embarked'])\n",
    "tprep.fit(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "\n",
       "   Parch     Fare Embarked  \n",
       "0      0   7.2500        S  \n",
       "1      0  71.2833        C  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   Age  SibSp  Parch     Fare  Sex_male  Pclass_2  Pclass_3  \\\n",
       "0         0  22.0      1      0   7.2500         1         0         1   \n",
       "1         1  38.0      1      0  71.2833         0         0         0   \n",
       "2         1  26.0      0      0   7.9250         0         0         1   \n",
       "3         1  35.0      1      0  53.1000         0         0         0   \n",
       "4         0  35.0      0      0   8.0500         1         0         1   \n",
       "\n",
       "   Embarked_Q  Embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tprep.transform(titanic).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the custom `TitanticPreprocessor` in a pipeline\n",
    "---\n",
    "\n",
    "We'll put it before the `StandardScaler` in our original pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['PassengerId', 'Name']\n",
    "columns_to_dummify = ['Sex', 'Pclass', 'Embarked']\n",
    "\n",
    "tprep = TitanticPreprocessor(columns_to_drop=columns_to_drop,\n",
    "                             columns_to_dummify=columns_to_dummify)\n",
    "scaler = StandardScaler()\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps=[('titanic_prep', tprep),\n",
    "                       ('scaler', scaler),\n",
    "                       ('model', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit on the training data and test on the testing data like before, with the new pipeline. You'll need to create a new X, y with the original non-manually preprocessed data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic.copy()\n",
    "y = X.pop('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7931427142714271\n",
      "0.8072289156626506\n",
      "0.8130841121495327\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "print(cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
    "print(pipe.score(X_train, y_train))\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.78610491, -0.31562099, -0.02758368,  0.05223936, -1.26550973,\n",
       "        -0.4648337 , -1.19487569, -0.21399548, -0.18777763]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['model'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at pipeline internals with `.get_params()`\n",
    "\n",
    "---\n",
    "\n",
    "Use the `.get_params()` function on the pipeline object to get out all of the parameters from the different steps as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('titanic_prep',\n",
       "   TitanticPreprocessor(columns_to_drop=['PassengerId', 'Name'],\n",
       "              columns_to_dummify=['Sex', 'Pclass', 'Embarked'])),\n",
       "  ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('model',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "             tol=0.0001, verbose=0, warm_start=False))],\n",
       " 'titanic_prep': TitanticPreprocessor(columns_to_drop=['PassengerId', 'Name'],\n",
       "            columns_to_dummify=['Sex', 'Pclass', 'Embarked']),\n",
       " 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " 'titanic_prep__columns_to_drop': ['PassengerId', 'Name'],\n",
       " 'titanic_prep__columns_to_dummify': ['Sex', 'Pclass', 'Embarked'],\n",
       " 'scaler__copy': True,\n",
       " 'scaler__with_mean': True,\n",
       " 'scaler__with_std': True,\n",
       " 'model__C': 1.0,\n",
       " 'model__class_weight': None,\n",
       " 'model__dual': False,\n",
       " 'model__fit_intercept': True,\n",
       " 'model__intercept_scaling': 1,\n",
       " 'model__max_iter': 100,\n",
       " 'model__multi_class': 'warn',\n",
       " 'model__n_jobs': None,\n",
       " 'model__penalty': 'l2',\n",
       " 'model__random_state': 1,\n",
       " 'model__solver': 'lbfgs',\n",
       " 'model__tol': 0.0001,\n",
       " 'model__verbose': 0,\n",
       " 'model__warm_start': False}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pull out the feature names we stored by accessing our preprocessor object from the dictionary, then pulling out the attribute from that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Pclass_2', 'Pclass_3',\n",
       "       'Embarked_Q', 'Embarked_S'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['titanic_prep'].feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `make_pipeline()` convenience function\n",
    "\n",
    "---\n",
    "\n",
    "`make_pipeline()` essentially does the same thing as `Pipeline`, the only difference being that you just insert your objects as arguments to the function and it will create the pipeline for you. This means that it will name the steps itself, rather than you doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_pipe = make_pipeline(\n",
    "    TitanticPreprocessor(columns_to_drop=columns_to_drop,\n",
    "                         columns_to_dummify=columns_to_dummify),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(solver='lbfgs', random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('titanticpreprocessor', TitanticPreprocessor(columns_to_drop=['PassengerId', 'Name'],\n",
       "           columns_to_dummify=['Sex', 'Pclass', 'Embarked'])), ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=Non..., penalty='l2', random_state=1, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8072289156626506\n",
      "0.8130841121495327\n"
     ]
    }
   ],
   "source": [
    "auto_pipe.fit(X_train, y_train)\n",
    "print(auto_pipe.score(X_train, y_train))\n",
    "print(auto_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Union\n",
    "\n",
    "Sometimes we want to give different treatments to different data columns. Some of the columns we might have to dummify, others might contain missing values for which we will choose a variety of imputation methods. This can be done in an efficient way with feature unions which can combine a variety of transformer objects.\n",
    "\n",
    "Like for the pipeline, there is an sklearn `FeatureUnion` which allows to set up all the transformation steps, but also a `make_union` which facilitates the set up.\n",
    "\n",
    "[sklearn feature union](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor\n",
    "\n",
    "First we create a class which does nothing else than extracting a column from a dataframe to return it as a matrix-like numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper class to extract features one by one in a pipeline\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.75 ],\n",
       "       [ 7.75 ],\n",
       "       [ 7.775],\n",
       "       [13.   ],\n",
       "       [12.475]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare_extractor = FeatureExtractor('Fare')\n",
    "fare_extractor.fit_transform(X_train)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dummifyer class\n",
    "\n",
    "This time however we are using the `LabelBinarizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = LabelBinarizer().fit(X).transform(X)\n",
    "        if X.shape[1] > 1:\n",
    "            return X[:, 1:]\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer = CustomBinarizer()\n",
    "binarizer.fit_transform(titanic['Embarked'])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the feature union\n",
    "\n",
    "Now for each column we will build a pipeline that extracts the indicated column, dummifies the column if required and imputes any missing values according to the specified method. \n",
    "\n",
    "Then we build the feature union which will be integrated into a pipeline with the standard scaler and the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to binarize labels and impute missing values with an appropriate method\n",
    "pclass_pipe = make_pipeline(\n",
    "    FeatureExtractor('Pclass'),\n",
    "    CustomBinarizer(),\n",
    "    SimpleImputer(strategy='most_frequent')\n",
    ")\n",
    "embarked_pipe = make_pipeline(\n",
    "    FeatureExtractor('Embarked'),\n",
    "    CustomBinarizer(),\n",
    "    SimpleImputer(strategy='most_frequent')\n",
    ")\n",
    "sex_pipe = make_pipeline(\n",
    "    FeatureExtractor('Sex'),\n",
    "    CustomBinarizer(),\n",
    "    SimpleImputer(strategy='most_frequent')\n",
    ")\n",
    "age_pipe = make_pipeline(\n",
    "    FeatureExtractor('Age'),\n",
    "    SimpleImputer(strategy='mean')\n",
    ")\n",
    "sibsp_pipe = make_pipeline(\n",
    "    FeatureExtractor('SibSp'),\n",
    "    SimpleImputer(strategy='most_frequent')\n",
    ")\n",
    "parch_pipe = make_pipeline(\n",
    "    FeatureExtractor('Parch'),\n",
    "    SimpleImputer(strategy='most_frequent')\n",
    ")\n",
    "fare_pipe = make_pipeline(\n",
    "    FeatureExtractor('Fare'),\n",
    "    SimpleImputer(strategy='most_frequent')\n",
    ")\n",
    "\n",
    "fu = make_union(pclass_pipe, sex_pipe, embarked_pipe,\n",
    "                age_pipe, sibsp_pipe, parch_pipe, fare_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fu_pipe.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = fu.fit_transform(X_train)\n",
    "test_X = fu.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  1.   ,  1.   ,  1.   ,  0.   , 40.5  ,  0.   ,  0.   ,\n",
       "         7.75 ],\n",
       "       [ 0.   ,  1.   ,  0.   ,  0.   ,  1.   , 22.   ,  0.   ,  0.   ,\n",
       "         7.75 ],\n",
       "       [ 0.   ,  1.   ,  0.   ,  0.   ,  1.   , 18.   ,  0.   ,  0.   ,\n",
       "         7.775],\n",
       "       [ 1.   ,  0.   ,  1.   ,  0.   ,  1.   , 48.   ,  0.   ,  0.   ,\n",
       "        13.   ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7931427142714271\n",
      "0.8072289156626506\n",
      "0.8130841121495327\n"
     ]
    }
   ],
   "source": [
    "fu_pipe = make_pipeline(fu, scaler, model)\n",
    "fu_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(cross_val_score(fu_pipe, X_train, y_train, cv=5).mean())\n",
    "print(fu_pipe.score(X_train, y_train))\n",
    "print(fu_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('featureunion', FeatureUnion(n_jobs=None,\n",
       "         transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "       steps=[('featureextractor', FeatureExtractor(column='Pclass')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "         strategy='most_frequent', verbose=0))])), ('pipel...ter(copy=True, fill_value=None, missing_values=nan,\n",
       "         strategy='most_frequent', verbose=0))]))],\n",
       "         transformer_weights=None)),\n",
       " ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " ('logisticregression',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "            tol=0.0001, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fu_pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Practice\n",
    "### Improve the model using grid search\n",
    "\n",
    "- Find out how to refer to the model tuning parameters in the pipeline (use `fu_pipe.get_params()`).\n",
    "- How would you modify your pipeline to use the `MinMaxScaler`?\n",
    "- What about using another model like kNN?\n",
    "- Create a transformer which returns the binarized version of a variable for being above or below a given threshold, e.g. for the Parch and SibSp columns.\n",
    "- Create polynomial features and/or interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridsearch with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None, 'steps': [('featureunion', FeatureUnion(n_jobs=None,\n",
       "          transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "        steps=[('featureextractor', FeatureExtractor(column='Pclass')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))])), ('pipel...ter(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))]))],\n",
       "          transformer_weights=None)),\n",
       "  ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('logisticregression',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "             tol=0.0001, verbose=0, warm_start=False))], 'featureunion': FeatureUnion(n_jobs=None,\n",
       "        transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "      steps=[('featureextractor', FeatureExtractor(column='Pclass')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0))])), ('pipel...ter(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0))]))],\n",
       "        transformer_weights=None), 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'logisticregression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=1, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0, warm_start=False), 'featureunion__n_jobs': None, 'featureunion__transformer_list': [('pipeline-1',\n",
       "   Pipeline(memory=None,\n",
       "        steps=[('featureextractor', FeatureExtractor(column='Pclass')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))])),\n",
       "  ('pipeline-2', Pipeline(memory=None,\n",
       "        steps=[('featureextractor', FeatureExtractor(column='Sex')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))])),\n",
       "  ('pipeline-3', Pipeline(memory=None,\n",
       "        steps=[('featureextractor', FeatureExtractor(column='Embarked')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))])),\n",
       "  ('pipeline-4', Pipeline(memory=None,\n",
       "        steps=[('featureextractor', FeatureExtractor(column='Age')), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "          verbose=0))])),\n",
       "  ('pipeline-5', Pipeline(memory=None,\n",
       "        steps=[('featureextractor', FeatureExtractor(column='SibSp')), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))])),\n",
       "  ('pipeline-6', Pipeline(memory=None,\n",
       "        steps=[('featureextractor', FeatureExtractor(column='Parch')), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))])),\n",
       "  ('pipeline-7', Pipeline(memory=None,\n",
       "        steps=[('featureextractor', FeatureExtractor(column='Fare')), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))]))], 'featureunion__transformer_weights': None, 'featureunion__pipeline-1': Pipeline(memory=None,\n",
       "      steps=[('featureextractor', FeatureExtractor(column='Pclass')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0))]), 'featureunion__pipeline-2': Pipeline(memory=None,\n",
       "      steps=[('featureextractor', FeatureExtractor(column='Sex')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0))]), 'featureunion__pipeline-3': Pipeline(memory=None,\n",
       "      steps=[('featureextractor', FeatureExtractor(column='Embarked')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0))]), 'featureunion__pipeline-4': Pipeline(memory=None,\n",
       "      steps=[('featureextractor', FeatureExtractor(column='Age')), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "        verbose=0))]), 'featureunion__pipeline-5': Pipeline(memory=None,\n",
       "      steps=[('featureextractor', FeatureExtractor(column='SibSp')), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0))]), 'featureunion__pipeline-6': Pipeline(memory=None,\n",
       "      steps=[('featureextractor', FeatureExtractor(column='Parch')), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0))]), 'featureunion__pipeline-7': Pipeline(memory=None,\n",
       "      steps=[('featureextractor', FeatureExtractor(column='Fare')), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0))]), 'featureunion__pipeline-1__memory': None, 'featureunion__pipeline-1__steps': [('featureextractor',\n",
       "   FeatureExtractor(column='Pclass')),\n",
       "  ('custombinarizer', CustomBinarizer()),\n",
       "  ('simpleimputer',\n",
       "   SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))], 'featureunion__pipeline-1__featureextractor': FeatureExtractor(column='Pclass'), 'featureunion__pipeline-1__custombinarizer': CustomBinarizer(), 'featureunion__pipeline-1__simpleimputer': SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0), 'featureunion__pipeline-1__featureextractor__column': 'Pclass', 'featureunion__pipeline-1__simpleimputer__copy': True, 'featureunion__pipeline-1__simpleimputer__fill_value': None, 'featureunion__pipeline-1__simpleimputer__missing_values': nan, 'featureunion__pipeline-1__simpleimputer__strategy': 'most_frequent', 'featureunion__pipeline-1__simpleimputer__verbose': 0, 'featureunion__pipeline-2__memory': None, 'featureunion__pipeline-2__steps': [('featureextractor',\n",
       "   FeatureExtractor(column='Sex')),\n",
       "  ('custombinarizer', CustomBinarizer()),\n",
       "  ('simpleimputer',\n",
       "   SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))], 'featureunion__pipeline-2__featureextractor': FeatureExtractor(column='Sex'), 'featureunion__pipeline-2__custombinarizer': CustomBinarizer(), 'featureunion__pipeline-2__simpleimputer': SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0), 'featureunion__pipeline-2__featureextractor__column': 'Sex', 'featureunion__pipeline-2__simpleimputer__copy': True, 'featureunion__pipeline-2__simpleimputer__fill_value': None, 'featureunion__pipeline-2__simpleimputer__missing_values': nan, 'featureunion__pipeline-2__simpleimputer__strategy': 'most_frequent', 'featureunion__pipeline-2__simpleimputer__verbose': 0, 'featureunion__pipeline-3__memory': None, 'featureunion__pipeline-3__steps': [('featureextractor',\n",
       "   FeatureExtractor(column='Embarked')),\n",
       "  ('custombinarizer', CustomBinarizer()),\n",
       "  ('simpleimputer',\n",
       "   SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))], 'featureunion__pipeline-3__featureextractor': FeatureExtractor(column='Embarked'), 'featureunion__pipeline-3__custombinarizer': CustomBinarizer(), 'featureunion__pipeline-3__simpleimputer': SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0), 'featureunion__pipeline-3__featureextractor__column': 'Embarked', 'featureunion__pipeline-3__simpleimputer__copy': True, 'featureunion__pipeline-3__simpleimputer__fill_value': None, 'featureunion__pipeline-3__simpleimputer__missing_values': nan, 'featureunion__pipeline-3__simpleimputer__strategy': 'most_frequent', 'featureunion__pipeline-3__simpleimputer__verbose': 0, 'featureunion__pipeline-4__memory': None, 'featureunion__pipeline-4__steps': [('featureextractor',\n",
       "   FeatureExtractor(column='Age')),\n",
       "  ('simpleimputer',\n",
       "   SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "          verbose=0))], 'featureunion__pipeline-4__featureextractor': FeatureExtractor(column='Age'), 'featureunion__pipeline-4__simpleimputer': SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "        verbose=0), 'featureunion__pipeline-4__featureextractor__column': 'Age', 'featureunion__pipeline-4__simpleimputer__copy': True, 'featureunion__pipeline-4__simpleimputer__fill_value': None, 'featureunion__pipeline-4__simpleimputer__missing_values': nan, 'featureunion__pipeline-4__simpleimputer__strategy': 'mean', 'featureunion__pipeline-4__simpleimputer__verbose': 0, 'featureunion__pipeline-5__memory': None, 'featureunion__pipeline-5__steps': [('featureextractor',\n",
       "   FeatureExtractor(column='SibSp')),\n",
       "  ('simpleimputer',\n",
       "   SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))], 'featureunion__pipeline-5__featureextractor': FeatureExtractor(column='SibSp'), 'featureunion__pipeline-5__simpleimputer': SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0), 'featureunion__pipeline-5__featureextractor__column': 'SibSp', 'featureunion__pipeline-5__simpleimputer__copy': True, 'featureunion__pipeline-5__simpleimputer__fill_value': None, 'featureunion__pipeline-5__simpleimputer__missing_values': nan, 'featureunion__pipeline-5__simpleimputer__strategy': 'most_frequent', 'featureunion__pipeline-5__simpleimputer__verbose': 0, 'featureunion__pipeline-6__memory': None, 'featureunion__pipeline-6__steps': [('featureextractor',\n",
       "   FeatureExtractor(column='Parch')),\n",
       "  ('simpleimputer',\n",
       "   SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))], 'featureunion__pipeline-6__featureextractor': FeatureExtractor(column='Parch'), 'featureunion__pipeline-6__simpleimputer': SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0), 'featureunion__pipeline-6__featureextractor__column': 'Parch', 'featureunion__pipeline-6__simpleimputer__copy': True, 'featureunion__pipeline-6__simpleimputer__fill_value': None, 'featureunion__pipeline-6__simpleimputer__missing_values': nan, 'featureunion__pipeline-6__simpleimputer__strategy': 'most_frequent', 'featureunion__pipeline-6__simpleimputer__verbose': 0, 'featureunion__pipeline-7__memory': None, 'featureunion__pipeline-7__steps': [('featureextractor',\n",
       "   FeatureExtractor(column='Fare')),\n",
       "  ('simpleimputer',\n",
       "   SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "          strategy='most_frequent', verbose=0))], 'featureunion__pipeline-7__featureextractor': FeatureExtractor(column='Fare'), 'featureunion__pipeline-7__simpleimputer': SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "        strategy='most_frequent', verbose=0), 'featureunion__pipeline-7__featureextractor__column': 'Fare', 'featureunion__pipeline-7__simpleimputer__copy': True, 'featureunion__pipeline-7__simpleimputer__fill_value': None, 'featureunion__pipeline-7__simpleimputer__missing_values': nan, 'featureunion__pipeline-7__simpleimputer__strategy': 'most_frequent', 'featureunion__pipeline-7__simpleimputer__verbose': 0, 'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'logisticregression__C': 1.0, 'logisticregression__class_weight': None, 'logisticregression__dual': False, 'logisticregression__fit_intercept': True, 'logisticregression__intercept_scaling': 1, 'logisticregression__max_iter': 100, 'logisticregression__multi_class': 'warn', 'logisticregression__n_jobs': None, 'logisticregression__penalty': 'l2', 'logisticregression__random_state': 1, 'logisticregression__solver': 'lbfgs', 'logisticregression__tol': 0.0001, 'logisticregression__verbose': 0, 'logisticregression__warm_start': False}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fu_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'standardscaler__with_std': [True, False],\n",
    "              'logisticregression__C': [1,2,3],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  30 out of  30 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "     steps=[('featureextractor', FeatureExtractor(column='Pclass')), ('custombinarizer', CustomBinarizer()), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,..., penalty='l2', random_state=1, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=2,\n",
       "       param_grid={'standardscaler__with_std': [True, False], 'logisticregression__C': [1, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gridsearch = GridSearchCV(fu_pipe,\n",
    "                              param_grid,\n",
    "                              n_jobs=2, \n",
    "                              cv=5, \n",
    "                              verbose=1, \n",
    "                              return_train_score=True)\n",
    "\n",
    "knn_gridsearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use different scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarize predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Now we can combine different preprocessing steps and models into a single pipeline making our code more fit for production environments. We can even create our classes which fit into the sklearn framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "- [Sklearn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)\n",
    "- [Sklearn feature union](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)\n",
    "- [Sklearn preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "- [Create your own estimator]( http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "170.667px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
